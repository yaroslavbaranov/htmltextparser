<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>Bibliography &#171; Deep Learning</title>
<link rel="shortcut icon" href="http://deeplearning.net/wp-content/ata-images/new-favicon.ico" />
<link rel="profile" href="http://gmpg.org/xfn/11" />
<link rel="pingback" href="http://deeplearning.net/xmlrpc.php" />
<link rel='dns-prefetch' href='//ajax.googleapis.com' />
<link rel='dns-prefetch' href='//cdn.datatables.net' />
<link rel='dns-prefetch' href='//cdn.rawgit.com' />
<link rel='dns-prefetch' href='//cdnjs.cloudflare.com' />
<link rel='dns-prefetch' href='//www.google.com' />
<link rel='dns-prefetch' href='//s.w.org' />
<link rel="alternate" type="application/rss+xml" title="Deep Learning &raquo; Feed" href="http://deeplearning.net/feed/" />
<link rel="alternate" type="application/rss+xml" title="Deep Learning &raquo; Comments Feed" href="http://deeplearning.net/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="Deep Learning &raquo; Bibliography Comments Feed" href="http://deeplearning.net/bibliography/feed/" />
<!-- This site uses the Google Analytics by MonsterInsights plugin v6.2.4 - Using Analytics tracking - https://www.monsterinsights.com/ -->
<script type="text/javascript" data-cfasync="false">
		var disableStr = 'ga-disable-UA-38366454-1';

	/* Function to detect opted out users */
	function __gaTrackerIsOptedOut() {
		return document.cookie.indexOf(disableStr + '=true') > -1;
	}

	/* Disable tracking if the opt-out cookie exists. */
	if ( __gaTrackerIsOptedOut() ) {
		window[disableStr] = true;
	}

	/* Opt-out function */
	function __gaTrackerOptout() {
	  document.cookie = disableStr + '=true; expires=Thu, 31 Dec 2099 23:59:59 UTC; path=/';
	  window[disableStr] = true;
	}
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','__gaTracker');

	__gaTracker('create', 'UA-38366454-1', 'auto');
	__gaTracker('set', 'forceSSL', true);
	__gaTracker('send','pageview');
</script>
<!-- / Google Analytics by MonsterInsights -->
		<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.3\/72x72\/","ext":".png","svgUrl":"https:\/\/s.w.org\/images\/core\/emoji\/2.3\/svg\/","svgExt":".svg","source":{"concatemoji":"http:\/\/deeplearning.net\/wp-includes\/js\/wp-emoji-release.min.js?ver=4.8.4"}};
			!function(a,b,c){function d(a){var b,c,d,e,f=String.fromCharCode;if(!k||!k.fillText)return!1;switch(k.clearRect(0,0,j.width,j.height),k.textBaseline="top",k.font="600 32px Arial",a){case"flag":return k.fillText(f(55356,56826,55356,56819),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,56826,8203,55356,56819),0,0),c=j.toDataURL(),b!==c&&(k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447),0,0),b=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447),0,0),c=j.toDataURL(),b!==c);case"emoji4":return k.fillText(f(55358,56794,8205,9794,65039),0,0),d=j.toDataURL(),k.clearRect(0,0,j.width,j.height),k.fillText(f(55358,56794,8203,9794,65039),0,0),e=j.toDataURL(),d!==e}return!1}function e(a){var c=b.createElement("script");c.src=a,c.defer=c.type="text/javascript",b.getElementsByTagName("head")[0].appendChild(c)}var f,g,h,i,j=b.createElement("canvas"),k=j.getContext&&j.getContext("2d");for(i=Array("flag","emoji4"),c.supports={everything:!0,everythingExceptFlag:!0},h=0;h<i.length;h++)c.supports[i[h]]=d(i[h]),c.supports.everything=c.supports.everything&&c.supports[i[h]],"flag"!==i[h]&&(c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&c.supports[i[h]]);c.supports.everythingExceptFlag=c.supports.everythingExceptFlag&&!c.supports.flag,c.DOMReady=!1,c.readyCallback=function(){c.DOMReady=!0},c.supports.everything||(g=function(){c.readyCallback()},b.addEventListener?(b.addEventListener("DOMContentLoaded",g,!1),a.addEventListener("load",g,!1)):(a.attachEvent("onload",g),b.attachEvent("onreadystatechange",function(){"complete"===b.readyState&&c.readyCallback()})),f=c.source||{},f.concatemoji?e(f.concatemoji):f.wpemoji&&f.twemoji&&(e(f.twemoji),e(f.wpemoji)))}(window,document,window._wpemojiSettings);
		</script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
<link rel='stylesheet' id='openid-css'  href='http://deeplearning.net/wp-content/plugins/openid/f/openid.css?ver=519' type='text/css' media='all' />
<link rel='stylesheet' id='jquery-datatables-css'  href='//cdn.datatables.net/1.10.12/css/jquery.dataTables.min.css?ver=4.8.4' type='text/css' media='all' />
<link rel='stylesheet' id='datatables-buttons-css'  href='//cdn.datatables.net/buttons/1.2.1/css/buttons.dataTables.min.css?ver=4.8.4' type='text/css' media='all' />
<link rel='stylesheet' id='datatables-select-css'  href='//cdn.datatables.net/select/1.2.0/css/select.dataTables.min.css?ver=4.8.4' type='text/css' media='all' />
<link rel='stylesheet' id='datatables-fixedheader-css'  href='//cdn.datatables.net/fixedheader/3.1.2/css/fixedHeader.dataTables.min.css?ver=4.8.4' type='text/css' media='all' />
<link rel='stylesheet' id='datatables-fixedcolumns-css'  href='//cdn.datatables.net/fixedcolumns/3.2.2/css/fixedColumns.dataTables.min.css?ver=4.8.4' type='text/css' media='all' />
<link rel='stylesheet' id='datatables-responsive-css'  href='//cdn.datatables.net/responsive/2.1.0/css/responsive.dataTables.min.css?ver=4.8.4' type='text/css' media='all' />
<script type='text/javascript' src='http://deeplearning.net/wp-includes/js/jquery/jquery.js?ver=1.12.4'></script>
<script type='text/javascript' src='http://deeplearning.net/wp-includes/js/jquery/jquery-migrate.min.js?ver=1.4.1'></script>
<script type='text/javascript' src='https://ajax.googleapis.com/ajax/libs/prototype/1.7.1.0/prototype.js?ver=1.7.1'></script>
<script type='text/javascript' src='http://deeplearning.net/wp-content/plugins/bib2html/js/bib2html.js?ver=0.7'></script>
<script type='text/javascript' src='http://deeplearning.net/wp-content/plugins/openid/f/openid.js?ver=519'></script>
<script type='text/javascript' src='https://ajax.googleapis.com/ajax/libs/scriptaculous/1.9.0/scriptaculous.js?ver=1.9.0'></script>
<script type='text/javascript' src='https://ajax.googleapis.com/ajax/libs/scriptaculous/1.9.0/effects.js?ver=1.9.0'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var monsterinsights_frontend = {"js_events_tracking":"true","is_debug_mode":"false","download_extensions":"","inbound_paths":"","home_url":"http:\/\/deeplearning.net","track_download_as":"event","internal_label":"int","hash_tracking":"false"};
/* ]]> */
</script>
<script type='text/javascript' src='http://deeplearning.net/wp-content/plugins/google-analytics-for-wordpress/assets/js/frontend.min.js?ver=6.2.4'></script>
<script type='text/javascript' src='//cdn.datatables.net/1.10.12/js/jquery.dataTables.min.js?ver=4.8.4'></script>
<script type='text/javascript' src='//cdn.datatables.net/buttons/1.2.1/js/dataTables.buttons.min.js?ver=4.8.4'></script>
<script type='text/javascript' src='//cdn.datatables.net/buttons/1.2.1/js/buttons.colVis.min.js?ver=4.8.4'></script>
<script type='text/javascript' src='//cdn.datatables.net/buttons/1.2.1/js/buttons.print.min.js?ver=4.8.4'></script>
<script type='text/javascript' src='//cdn.rawgit.com/bpampuch/pdfmake/0.1.18/build/pdfmake.min.js?ver=4.8.4'></script>
<script type='text/javascript' src='//cdn.rawgit.com/bpampuch/pdfmake/0.1.18/build/vfs_fonts.js?ver=4.8.4'></script>
<script type='text/javascript' src='//cdnjs.cloudflare.com/ajax/libs/jszip/2.5.0/jszip.min.js?ver=4.8.4'></script>
<script type='text/javascript' src='//cdn.datatables.net/buttons/1.2.1/js/buttons.html5.min.js?ver=4.8.4'></script>
<script type='text/javascript' src='//cdn.datatables.net/select/1.2.0/js/dataTables.select.min.js?ver=4.8.4'></script>
<script type='text/javascript' src='//cdn.datatables.net/fixedheader/3.1.2/js/dataTables.fixedHeader.min.js?ver=4.8.4'></script>
<script type='text/javascript' src='//cdn.datatables.net/fixedcolumns/3.2.2/js/dataTables.fixedColumns.min.js?ver=4.8.4'></script>
<script type='text/javascript' src='//cdn.datatables.net/responsive/2.1.0/js/dataTables.responsive.min.js?ver=4.8.4'></script>
<script type='text/javascript'>
/* <![CDATA[ */
var igsv_plugin_vars = {"lang_dir":"http:\/\/deeplearning.net\/wp-content\/plugins\/inline-google-spreadsheet-viewer\/languages","datatables_classes":".igsv-table:not(.no-datatables)","datatables_defaults_object":{"dom":"TC<'clear'>lfrtip"}};
/* ]]> */
</script>
<script type='text/javascript' src='http://deeplearning.net/wp-content/plugins/inline-google-spreadsheet-viewer/igsv-datatables.js?ver=4.8.4'></script>
<script type='text/javascript' src='//www.google.com/jsapi?ver=4.8.4'></script>
<script type='text/javascript' src='http://deeplearning.net/wp-content/plugins/inline-google-spreadsheet-viewer/igsv-gvizcharts.js?ver=4.8.4'></script>
<script type='text/javascript' src='http://deeplearning.net/wp-content/themes/atahualpa/js/DD_roundies.js?ver=0.0.2a'></script>
<link rel='https://api.w.org/' href='http://deeplearning.net/wp-json/' />
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://deeplearning.net/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://deeplearning.net/wp-includes/wlwmanifest.xml" /> 
<link rel="canonical" href="http://deeplearning.net/bibliography/" />
<link rel='shortlink' href='http://deeplearning.net/?p=157' />
<link rel="alternate" type="application/json+oembed" href="http://deeplearning.net/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fdeeplearning.net%2Fbibliography%2F" />
<link rel="alternate" type="text/xml+oembed" href="http://deeplearning.net/wp-json/oembed/1.0/embed?url=http%3A%2F%2Fdeeplearning.net%2Fbibliography%2F&#038;format=xml" />
			<style type="text/css">
			a.backlinks_title{
				font-style:bold;
				font-size:150%
			}
			a.backlinks_title:hover{
				text-decoration:underline;
			}
			a.morelink{
				font-style:italic;
				font-size:75%
				color:#888;
				text-decoration:none;
			}
			ul.dates{
				list-style-type:none;
				margin:1.5em 0 2em 0;
				border-top:1px solid #3D3D3D;
			}
			ul.dates .date{
				color:#858585;
				padding:0 1.5em 0 0;
			}
			</style><style type="text/css">

div.bibtex {
    display: none;
}</style><script type="text/javascript">
(function(url){
	if(/(?:Chrome\/26\.0\.1410\.63 Safari\/537\.31|WordfenceTestMonBot)/.test(navigator.userAgent)){ return; }
	var addEvent = function(evt, handler) {
		if (window.addEventListener) {
			document.addEventListener(evt, handler, false);
		} else if (window.attachEvent) {
			document.attachEvent('on' + evt, handler);
		}
	};
	var removeEvent = function(evt, handler) {
		if (window.removeEventListener) {
			document.removeEventListener(evt, handler, false);
		} else if (window.detachEvent) {
			document.detachEvent('on' + evt, handler);
		}
	};
	var evts = 'contextmenu dblclick drag dragend dragenter dragleave dragover dragstart drop keydown keypress keyup mousedown mousemove mouseout mouseover mouseup mousewheel scroll'.split(' ');
	var logHuman = function() {
		var wfscr = document.createElement('script');
		wfscr.type = 'text/javascript';
		wfscr.async = true;
		wfscr.src = url + '&r=' + Math.random();
		(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(wfscr);
		for (var i = 0; i < evts.length; i++) {
			removeEvent(evts[i], logHuman);
		}
	};
	for (var i = 0; i < evts.length; i++) {
		addEvent(evts[i], logHuman);
	}
})('//deeplearning.net/?wordfence_logHuman=1&hid=80152AE3E12D81AEDD79815D6055F0D0');
</script><style data-context="foundation-flickity-css">/*! Flickity v2.0.2
http://flickity.metafizzy.co
---------------------------------------------- */.flickity-enabled{position:relative}.flickity-enabled:focus{outline:0}.flickity-viewport{overflow:hidden;position:relative;height:100%}.flickity-slider{position:absolute;width:100%;height:100%}.flickity-enabled.is-draggable{-webkit-tap-highlight-color:transparent;tap-highlight-color:transparent;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.flickity-enabled.is-draggable .flickity-viewport{cursor:move;cursor:-webkit-grab;cursor:grab}.flickity-enabled.is-draggable .flickity-viewport.is-pointer-down{cursor:-webkit-grabbing;cursor:grabbing}.flickity-prev-next-button{position:absolute;top:50%;width:44px;height:44px;border:none;border-radius:50%;background:#fff;background:hsla(0,0%,100%,.75);cursor:pointer;-webkit-transform:translateY(-50%);transform:translateY(-50%)}.flickity-prev-next-button:hover{background:#fff}.flickity-prev-next-button:focus{outline:0;box-shadow:0 0 0 5px #09F}.flickity-prev-next-button:active{opacity:.6}.flickity-prev-next-button.previous{left:10px}.flickity-prev-next-button.next{right:10px}.flickity-rtl .flickity-prev-next-button.previous{left:auto;right:10px}.flickity-rtl .flickity-prev-next-button.next{right:auto;left:10px}.flickity-prev-next-button:disabled{opacity:.3;cursor:auto}.flickity-prev-next-button svg{position:absolute;left:20%;top:20%;width:60%;height:60%}.flickity-prev-next-button .arrow{fill:#333}.flickity-page-dots{position:absolute;width:100%;bottom:-25px;padding:0;margin:0;list-style:none;text-align:center;line-height:1}.flickity-rtl .flickity-page-dots{direction:rtl}.flickity-page-dots .dot{display:inline-block;width:10px;height:10px;margin:0 8px;background:#333;border-radius:50%;opacity:.25;cursor:pointer}.flickity-page-dots .dot.is-selected{opacity:1}</style><style data-context="foundation-slideout-css">.slideout-menu{position:fixed;left:0;top:0;bottom:0;right:auto;z-index:0;width:256px;overflow-y:auto;-webkit-overflow-scrolling:touch;display:none}.slideout-menu.pushit-right{left:auto;right:0}.slideout-panel{position:relative;z-index:1;will-change:transform}.slideout-open,.slideout-open .slideout-panel,.slideout-open body{overflow:hidden}.slideout-open .slideout-menu{display:block}.pushit{display:none}</style><style type="text/css">body{text-align:center;margin:0;padding:0;font-family:tahoma,arial,sans-serif;font-size:0.8em;color:#000000;background:#ffffff}a:link,a:visited,a:active{color:#666666;font-weight:bold;text-decoration:none;}a:hover{color:#CC0000;font-weight:bold;text-decoration:underline}ul,ol,dl,p,h1,h2,h3,h4,h5,h6{margin-top:10px;margin-bottom:10px;padding-top:0;padding-bottom:0;}ul ul,ul ol,ol ul,ol ol{margin-top:0;margin-bottom:0}code,pre{font-family:"Courier New",Courier,monospace;font-size:1em}pre{overflow:auto;word-wrap:normal;padding-bottom:1.5em;overflow-y:hidden;width:99%}abbr[title],acronym[title]{border-bottom:1px dotted}hr{display:block;height:2px;border:none;margin:0.5em auto;color:#cccccc;background-color:#cccccc}table{font-size:1em;}div.post,ul.commentlist li,ol.commentlist li{word-wrap:break-word;}pre,.wp_syntax{word-wrap:normal;}div#wrapper{text-align:center;margin-left:auto;margin-right:auto;display:block;width:99%}div#container{padding:0;width:auto;margin-left:auto;margin-right:auto;text-align:left;display:block}table#layout{font-size:100%;width:100%;table-layout:fixed}.colone{width:200px}.colone-inner{width:200px}.coltwo{width:100% }.colthree-inner{width:200px}.colthree{width:200px}div#header.full-width{width:100%}div#header,td#header{width:auto;padding:0}table#logoarea,table#logoarea tr,table#logoarea td{margin:0;padding:0;background:none;border:0}table#logoarea{width:100%;border-spacing:0px}img.logo{display:block;margin:0 10px 0 0}td.logoarea-logo{width:1%}h1.blogtitle,h2.blogtitle{ display:block;margin:0;padding:0;letter-spacing:-1px;line-height:1.0em;font-family:‘Lucida Sans Unicode’,‘Lucida Grande’,sans-serif;font-size:240%;font-smooth:always}h1.blogtitle a:link,h1.blogtitle a:visited,h1.blogtitle a:active,h2.blogtitle a:link,h2.blogtitle a:visited,h2.blogtitle a:active{ text-decoration:none;color:#666666;font-weight:bold;font-smooth:always}h1.blogtitle a:hover,h2.blogtitle a:hover{ text-decoration:none;color:#000000;font-weight:bold}p.tagline{margin:0;padding:0;font-size:1.2em;font-weight:bold;color:#666666}td.feed-icons{white-space:nowrap;}div.rss-box{height:1%;display:block;padding:10px 0 10px 10px;margin:0;width:280px}a.comments-icon{height:22px;line-height:22px;margin:0 5px 0 5px;padding-left:22px;display:block;text-decoration:none;float:right;white-space:nowrap}a.comments-icon:link,a.comments-icon:active,a.comments-icon:visited{background:transparent url(http://deeplearning.net/wp-content/themes/atahualpa/images/comment-gray.png) no-repeat scroll center left}a.comments-icon:hover{background:transparent url(http://deeplearning.net/wp-content/themes/atahualpa/images/comment.png) no-repeat scroll center left}a.posts-icon{height:22px;line-height:22px;margin:0 5px 0 0;padding-left:20px;display:block;text-decoration:none;float:right;white-space:nowrap}a.posts-icon:link,a.posts-icon:active,a.posts-icon:visited{background:transparent url(http://deeplearning.net/wp-content/themes/atahualpa/images/rss-gray.png) no-repeat scroll center left}a.posts-icon:hover{background:transparent url(http://deeplearning.net/wp-content/themes/atahualpa/images/rss.png) no-repeat scroll center left}a.email-icon{height:22px;line-height:22px;margin:0 5px 0 5px;padding-left:24px;display:block;text-decoration:none;float:right;white-space:nowrap}a.email-icon:link,a.email-icon:active,a.email-icon:visited{background:transparent url(http://deeplearning.net/wp-content/themes/atahualpa/images/email-gray.png) no-repeat scroll center left}a.email-icon:hover{background:transparent url(http://deeplearning.net/wp-content/themes/atahualpa/images/email.png) no-repeat scroll center left}td.search-box{height:1%}div.searchbox{height:35px;border:1px dashed #cccccc;border-bottom:0;width:200px;margin:0;padding:0}div.searchbox-form{margin:5px 10px 5px 10px}div.horbar1,div.horbar2{font-size:1px;clear:both;display:block;position:relative;padding:0;margin:0}div.horbar1{height:5px;background:#ffffff;border-top:dashed 1px #cccccc}div.horbar2{height:5px;background:#ffffff;border-bottom:dashed 1px #cccccc}div.header-image-container-pre{position:relative;margin:0;padding:0;height:100px;}div.header-image-container{position:relative;margin:0;padding:0;height:100px;}div.codeoverlay{position:absolute;top:0;left:0;width:100%;height:100%}div.opacityleft{position:absolute;z-index:2;top:0;left:0;background-color:#FFFFFF;height:100px;width:200px;filter:alpha(opacity=40);opacity:.40}div.opacityright{position:absolute;z-index:2;top:0;right:0;background-color:#FFFFFF;height:100px;width:200px;filter:alpha(opacity=40);opacity:.40}a.divclick:link,a.divclick:visited,a.divclick:active,a.divclick:hover{width:100%;height:100%;display:block;text-decoration:none}td#left{vertical-align:top;border-right:dashed 1px #CCCCCC;padding:10px 10px 10px 10px;background:#ffffff}td#left-inner{vertical-align:top;border-right:dashed 1px #CCCCCC;padding:10px 10px 10px 10px;background:#ffffff}td#right{vertical-align:top;border-left:dashed 1px #CCCCCC;padding:10px 10px 10px 10px;background:#ffffff}td#right-inner{vertical-align:top;border-left:dashed 1px #CCCCCC;padding:10px 10px 10px 10px;background:#ffffff}td#middle{vertical-align:top;width:100%;padding:10px 15px}div#footer.full-width{width:100%}div#footer,td#footer{width:auto;background-color:#ffffff;border-top:dashed 1px #cccccc;padding:10px;text-align:center;color:#777777;font-size:95%}div#footer a:link,div#footer a:visited,div#footer a:active,td#footer a:link,td#footer a:visited,td#footer a:active{text-decoration:none;color:#777777;font-weight:normal}div#footer a:hover,td#footer a:hover{text-decoration:none;color:#777777;font-weight:normal}div.widget{display:block;width:auto;margin:0 0 15px 0}div.widget-title{display:block;width:auto}div.widget-title h3,td#left h3.tw-widgettitle,td#right h3.tw-widgettitle,td#left ul.tw-nav-list,td#right ul.tw-nav-list{padding:0;margin:0;font-size:1.6em;font-weight:bold}div.widget ul,div.textwidget{display:block;width:auto}div.widget select{width:98%;margin-top:5px;}div.widget ul{list-style-type:none;margin:0;padding:0;width:auto}div.widget ul li{display:block;margin:2px 0 2px 0px;padding:0 0 0 5px;border-left:solid 7px #cccccc}div.widget ul li:hover,div.widget ul li.sfhover{display:block;width:auto;border-left:solid 7px #000000;}div.widget ul li ul li{margin:2px 0 2px 5px;padding:0 0 0 5px;border-left:solid 7px #cccccc;}div.widget ul li ul li:hover,div.widget ul li ul li.sfhover{border-left:solid 7px #000000;}div.widget ul li ul li ul li{margin:2px 0 2px 5px;padding:0 0 0 5px;border-left:solid 7px #cccccc;}div.widget ul li ul li ul li:hover,div.widget ul li ul li ul li.sfhover{border-left:solid 7px #000000;}div.widget a:link,div.widget a:visited,div.widget a:active,div.widget td a:link,div.widget td a:visited,div.widget td a:active,div.widget ul li a:link,div.widget ul li a:visited,div.widget ul li a:active{text-decoration:none;font-weight:normal;color:#666666;font-weight:normal;}div.widget ul li ul li a:link,div.widget ul li ul li a:visited,div.widget ul li ul li a:active{color:#666666;font-weight:normal;}div.widget ul li ul li ul li a:link,div.widget ul li ul li ul li a:visited,div.widget ul li ul li ul li a:active{color:#666666;font-weight:normal;}div.widget a:hover,div.widget ul li a:hover{color:#000000;}div.widget ul li ul li a:hover{color:#000000;}div.widget ul li ul li ul li a:hover{color:#000000;}div.widget ul li a:link,div.widget ul li a:visited,div.widget ul li a:active,div.widget ul li a:hover{display:inline}* html div.widget ul li a:link,* html div.widget ul li a:visited,* html div.widget ul li a:active,* html div.widget ul li a:hover{height:1%; } div.widget_nav_menu ul li,div.widget_pages ul li,div.widget_categories ul li{border-left:0 !important;padding:0 !important}div.widget_nav_menu ul li a:link,div.widget_nav_menu ul li a:visited,div.widget_nav_menu ul li a:active,div.widget_pages ul li a:link,div.widget_pages ul li a:visited,div.widget_pages ul li a:active,div.widget_categories ul li a:link,div.widget_categories ul li a:visited,div.widget_categories ul li a:active{padding:0 0 0 5px;border-left:solid 7px #cccccc}div.widget_nav_menu ul li a:hover,div.widget_pages ul li a:hover,div.widget_categories ul li a:hover{border-left:solid 7px #000000;}div.widget_nav_menu ul li ul li a:link,div.widget_nav_menu ul li ul li a:visited,div.widget_nav_menu ul li ul li a:active,div.widget_pages ul li ul li a:link,div.widget_pages ul li ul li a:visited,div.widget_pages ul li ul li a:active,div.widget_categories ul li ul li a:link,div.widget_categories ul li ul li a:visited,div.widget_categories ul li ul li a:active{padding:0 0 0 5px;border-left:solid 7px #cccccc}div.widget_nav_menu ul li ul li a:hover,div.widget_pages ul li ul li a:hover,div.widget_categories ul li ul li a:hover{border-left:solid 7px #000000;}div.widget_nav_menu ul li ul li ul li a:link,div.widget_nav_menu ul li ul li ul li a:visited,div.widget_nav_menu ul li ul li ul li a:active,div.widget_pages ul li ul li ul li a:link,div.widget_pages ul li ul li ul li a:visited,div.widget_pages ul li ul li ul li a:active,div.widget_categories ul li ul li ul li a:link,div.widget_categories ul li ul li ul li a:visited,div.widget_categories ul li ul li ul li a:active{padding:0 0 0 5px;border-left:solid 7px #cccccc}div.widget_nav_menu ul li ul li ul li a:hover,div.widget_pages ul li ul li ul li a:hover,div.widget_categories ul li ul li ul li a:hover{border-left:solid 7px #000000;}div.widget_nav_menu ul li a:link,div.widget_nav_menu ul li a:active,div.widget_nav_menu ul li a:visited,div.widget_nav_menu ul li a:hover,div.widget_pages ul li a:link,div.widget_pages ul li a:active,div.widget_pages ul li a:visited,div.widget_pages ul li a:hover{display:block !important}div.widget_categories ul li a:link,div.widget_categories ul li a:active,div.widget_categories ul li a:visited,div.widget_categories ul li a:hover{display:inline !important}table.subscribe{width:100%}table.subscribe td.email-text{padding:0 0 5px 0;vertical-align:top}table.subscribe td.email-field{padding:0;width:100%}table.subscribe td.email-button{padding:0 0 0 5px}table.subscribe td.post-text{padding:7px 0 0 0;vertical-align:top}table.subscribe td.comment-text{padding:7px 0 0 0;vertical-align:top}div.post,div.page{display:block;margin:0 0 30px 0}div.sticky{background:#eee url(http://deeplearning.net/wp-content/themes/atahualpa/images/sticky.gif) 99% 5% no-repeat;border:dashed 1px #cccccc;padding:10px}div.post-kicker{margin:0 0 5px 0}div.post-kicker a:link,div.post-kicker a:visited,div.post-kicker a:active{color:#000000;text-decoration:none;text-transform:uppercase}div.post-kicker a:hover{color:#cc0000}div.post-headline{}div.post-headline h1,div.post-headline h2{ margin:0; padding:0;padding:0;margin:0}div.post-headline h2 a:link,div.post-headline h2 a:visited,div.post-headline h2 a:active,div.post-headline h1 a:link,div.post-headline h1 a:visited,div.post-headline h1 a:active{color:#666666;text-decoration:none}div.post-headline h2 a:hover,div.post-headline h1 a:hover{color:#000000;text-decoration:none}div.post-byline{margin:5px 0 10px 0}div.post-byline a:link,div.post-byline a:visited,div.post-byline a:active{}div.post-byline a:hover{}div.post-bodycopy{}div.post-bodycopy p{margin:1em 0;padding:0;display:block}div.post-pagination{}div.post-footer{clear:both;display:block;margin:0;padding:5px;background:#eeeeee;color:#666;line-height:18px}div.post-footer a:link,div.post-footer a:visited,div.post-footer a:active{color:#333;font-weight:normal;text-decoration:none}div.post-footer a:hover{color:#333;font-weight:normal;text-decoration:underline}div.post-kicker img,div.post-byline img,div.post-footer img{border:0;padding:0;margin:0 0 -1px 0;background:none}span.post-ratings{display:inline-block;width:auto;white-space:nowrap}div.navigation-top{margin:0 0 10px 0;padding:0 0 10px 0;border-bottom:dashed 1px #cccccc}div.navigation-middle{margin:10px 0 20px 0;padding:10px 0 10px 0;border-top:dashed 1px #cccccc;border-bottom:dashed 1px #cccccc}div.navigation-bottom{margin:20px 0 0 0;padding:10px 0 0 0;border-top:dashed 1px #cccccc}div.navigation-comments-above{margin:0 0 10px 0;padding:5px 0 5px 0}div.navigation-comments-below{margin:0 0 10px 0;padding:5px 0 5px 0}div.older{float:left;width:48%;text-align:left;margin:0;padding:0}div.newer{float:right;width:48%;text-align:right;margin:0;padding:0;}div.older-home{float:left;width:44%;text-align:left;margin:0;padding:0}div.newer-home{float:right;width:44%;text-align:right;margin:0;padding:0;}div.home{float:left;width:8%;text-align:center;margin:0;padding:0}form,.feedburner-email-form{margin:0;padding:0;}fieldset{border:1px solid #cccccc;width:auto;padding:0.35em 0.625em 0.75em;display:block;}legend{color:#000000;background:#f4f4f4;border:1px solid #cccccc;padding:2px 6px;margin-bottom:15px;}form p{margin:5px 0 0 0;padding:0;}div.xhtml-tags p{margin:0}label{margin-right:0.5em;font-family:arial;cursor:pointer;}input.text,input.textbox,input.password,input.file,input.TextField,textarea{padding:3px;color:#000000;border-top:solid 1px #333333;border-left:solid 1px #333333;border-right:solid 1px #999999;border-bottom:solid 1px #cccccc;background:url(http://deeplearning.net/wp-content/themes/atahualpa/images/inputbackgr.gif) top left no-repeat}textarea{width:96%;}input.inputblur{color:#777777;width:95%}input.inputfocus{color:#000000;width:95%}input.highlight,textarea.highlight{background:#e8eff7;border-color:#37699f}.button,.Button,input[type=submit]{padding:0 2px;height:24px;line-height:16px;background-color:#777777;color:#ffffff;border:solid 2px #555555;font-weight:bold}input.buttonhover{padding:0 2px;cursor:pointer;background-color:#6b9c6b;color:#ffffff;border:solid 2px #496d49}form#commentform input#submit{ padding:0 .25em; overflow:visible}form#commentform input#submit[class]{width:auto}form#commentform input#submit{padding:4px 10px 4px 10px;font-size:1.2em;line-height:1.5em;height:36px}table.searchform{width:100%}table.searchform td.searchfield{padding:0;width:100%}table.searchform td.searchbutton{padding:0 0 0 5px}table.searchform td.searchbutton input{padding:0 0 0 5px}blockquote{height:1%;display:block;clear:both;color:#555555;padding:1em 1em;background:#f4f4f4;border:solid 1px #e1e1e1}blockquote blockquote{height:1%;display:block;clear:both;color:#444444;padding:1em 1em;background:#e1e1e1;border:solid 1px #d3d3d3}div.post table{border-collapse:collapse;margin:10px 0}div.post table caption{width:auto;margin:0 auto;background:#eeeeee;border:#999999;padding:4px 8px;color:#666666}div.post table th{background:#888888;color:#ffffff;font-weight:bold;font-size:90%;padding:4px 8px;border:solid 1px #ffffff;text-align:left}div.post table td{padding:4px 8px;background-color:#ffffff;border-bottom:1px solid #dddddd;text-align:left}div.post table tfoot td{}div.post table tr.alt td{background:#f4f4f4}div.post table tr.over td{background:#e2e2e2}#calendar_wrap{padding:0;border:none}table#wp-calendar{width:100%;font-size:90%;border-collapse:collapse;background-color:#ffffff;margin:0 auto}table#wp-calendar caption{width:auto;background:#eeeeee;border:none;padding:3px;margin:0 auto;font-size:1em}table#wp-calendar th{border:solid 1px #eeeeee;background-color:#999999;color:#ffffff;font-weight:bold;padding:2px;text-align:center}table#wp-calendar td{padding:0;line-height:18px;background-color:#ffffff;border:1px solid #dddddd;text-align:center}table#wp-calendar tfoot td{border:solid 1px #eeeeee;background-color:#eeeeee}table#wp-calendar td a{display:block;background-color:#eeeeee;width:100%;height:100%;padding:0}div#respond{margin:25px 0;padding:25px;background:#eee;-moz-border-radius:8px;-khtml-border-radius:8px;-webkit-border-radius:8px;border-radius:8px}p.thesetags{margin:10px 0}h3.reply,h3#reply-title{margin:0;padding:0 0 10px 0}ol.commentlist{margin:15px 0 25px 0;list-style-type:none;padding:0;display:block;border-top:dotted 1px #cccccc}ol.commentlist li{padding:15px 10px;display:block;height:1%;margin:0;background-color:#ffffff;border-bottom:dotted 1px #cccccc}ol.commentlist li.alt{display:block;height:1%;background-color:#eeeeee;border-bottom:dotted 1px #cccccc}ol.commentlist li.authorcomment{display:block;height:1%;background-color:#ffecec}ol.commentlist span.authorname{font-weight:bold;font-size:110%}ol.commentlist span.commentdate{color:#666666;font-size:90%;margin-bottom:5px;display:block}ol.commentlist span.editcomment{display:block}ol.commentlist li p{margin:2px 0 5px 0}div.comment-number{float:right;font-size:2em;line-height:2em;font-family:georgia,serif;font-weight:bold;color:#ddd;margin:-10px 0 0 0;position:relative;height:1%}div.comment-number a:link,div.comment-number a:visited,div.comment-number a:active{color:#ccc}textarea#comment{width:98%;margin:10px 0;display:block}ul.commentlist{margin:15px 0 15px 0;list-style-type:none;padding:0;display:block;border-top:dotted 1px #cccccc}ul.commentlist ul{margin:0;border:none;list-style-type:none;padding:0}ul.commentlist li{padding:0; margin:0;display:block;clear:both;height:1%;}ul.commentlist ul.children li{ margin-left:30px}ul.commentlist div.comment-container{padding:10px;margin:0}ul.children div.comment-container{background-color:transparent;border:dotted 1px #ccc;padding:10px;margin:0 10px 8px 0; border-radius:5px}ul.children div.bypostauthor{}ul.commentlist li.thread-even{background-color:#ffffff;border-bottom:dotted 1px #cccccc}ul.commentlist li.thread-odd{background-color:#eeeeee;border-bottom:dotted 1px #cccccc}ul.commentlist div.bypostauthor{background-color:#ffecec}ul.children div.bypostauthor{border:dotted 1px #ffbfbf}ul.commentlist span.authorname{font-size:110%}div.comment-meta a:link,div.comment-meta a:visited,div.comment-meta a:active,div.comment-meta a:hover{font-weight:normal}div#cancel-comment-reply{margin:-5px 0 10px 0}div.comment-number{float:right;font-size:2em;line-height:2em;font-family:georgia,serif;font-weight:bold;color:#ddd;margin:-10px 0 0 0;position:relative;height:1%}div.comment-number a:link,div.comment-number a:visited,div.comment-number a:active{color:#ccc}.page-numbers{padding:2px 6px;border:solid 1px #000000;border-radius:6px}span.current{background:#ddd}a.prev,a.next{border:none}a.page-numbers:link,a.page-numbers:visited,a.page-numbers:active{text-decoration:none;color:#666666;border-color:#666666}a.page-numbers:hover{text-decoration:none;color:#CC0000;border-color:#CC0000}div.xhtml-tags{display:none}abbr em{border:none !important;border-top:dashed 1px #aaa !important;display:inline-block !important;background:url(http://deeplearning.net/wp-content/themes/atahualpa/images/commentluv.gif) 0% 90% no-repeat;margin-top:8px;padding:5px 5px 2px 20px !important;font-style:normal}p.subscribe-to-comments{margin-bottom:10px}div#gsHeader{display:none;}div.g2_column{margin:0 !important;width:100% !important;font-size:1.2em}div#gsNavBar{border-top-width:0 !important}p.giDescription{font-size:1.2em;line-height:1 !important}p.giTitle{margin:0.3em 0 !important;font-size:1em;font-weight:normal;color:#666}div#wp-email img{border:0;padding:0}div#wp-email input,div#wp-email textarea{margin-top:5px;margin-bottom:2px}div#wp-email p{margin-bottom:10px}input#wp-email-submit{ padding:0; font-size:30px; height:50px; line-height:50px; overflow:visible;}img.WP-EmailIcon{ vertical-align:text-bottom !important}.tw-accordion .tw-widgettitle,.tw-accordion .tw-widgettitle:hover,.tw-accordion .tw-hovered,.tw-accordion .selected,.tw-accordion .selected:hover{ background:transparent !important; background-image:none !important}.tw-accordion .tw-widgettitle span{ padding-left:0 !important}.tw-accordion h3.tw-widgettitle{border-bottom:solid 1px #ccc}.tw-accordion h3.selected{border-bottom:none}td#left .without_title,td#right .without_title{ margin-top:0;margin-bottom:0}ul.tw-nav-list{border-bottom:solid 1px #999;display:block;margin-bottom:5px !important}td#left ul.tw-nav-list li,td#right ul.tw-nav-list li{padding:0 0 1px 0;margin:0 0 -1px 5px; border:solid 1px #ccc;border-bottom:none;border-radius:5px;border-bottom-right-radius:0;border-bottom-left-radius:0;background:#eee}td#left ul.tw-nav-list li.ui-tabs-selected,td#right ul.tw-nav-list li.ui-tabs-selected{ background:none;border:solid 1px #999;border-bottom:solid 1px #fff !important}ul.tw-nav-list li a:link,ul.tw-nav-list li a:visited,ul.tw-nav-list li a:active,ul.tw-nav-list li a:hover{padding:0 8px !important;background:none;border-left:none !important;outline:none}td#left ul.tw-nav-list li.ui-tabs-selected a,td#left li.ui-tabs-selected a:hover,td#right ul.tw-nav-list li.ui-tabs-selected a,td#right li.ui-tabs-selected a:hover{ color:#000000; text-decoration:none;font-weight:bold;background:none !important;outline:none}td#left .ui-tabs-panel,td#right .ui-tabs-panel{ margin:0; padding:0}img{border:0}#dbem-location-map img{ background:none !important}.post img{padding:0px;border:solid 0px #dddddd;background-color:#f3f3f3;-moz-border-radius:0px;-khtml-border-radius:0px;-webkit-border-radius:0px;border-radius:0px}.post img.size-full{max-width:96%;width:auto;margin:5px 0 5px 0}div.post img[class~=size-full]{height:auto;}.post img.alignleft{float:left;margin:10px 10px 5px 0;}.post img.alignright{float:right;margin:10px 0 5px 10px;}.post img.aligncenter{display:block;margin:10px auto}.aligncenter,div.aligncenter{ display:block; margin-left:auto; margin-right:auto}.alignleft,div.alignleft{float:left;margin:10px 10px 5px 0}.alignright,div.alignright{ float:right; margin:10px 0 5px 10px}div.archives-page img{border:0;padding:0;background:none;margin-bottom:0;vertical-align:-10%}.wp-caption{max-width:96%;width:auto 100%;height:auto;display:block;border:0px solid #dddddd;text-align:center;background-color:#f3f3f3;padding-top:0px;margin:0px 0 0 0;-moz-border-radius:0px;-khtml-border-radius:0px;-webkit-border-radius:0px;border-radius:0px}* html .wp-caption{height:100%;}.wp-caption img{ margin:0 !important; padding:0 !important; border:0 none !important}.wp-caption-text,.wp-caption p.wp-caption-text{font-size:0.8em;line-height:13px;padding:2px 4px 5px;margin:0;color:#666666}img.wp-post-image{float:left;border:0;padding:0;background:none;margin:0 10px 5px 0}img.wp-smiley{ float:none;border:none !important;margin:0 1px -1px 1px;padding:0 !important;background:none !important}img.avatar{float:left;display:block;margin:0 8px 1px 0;padding:3px;border:solid 1px #ddd;background-color:#f3f3f3;-moz-border-radius:3px;-khtml-border-radius:3px;-webkit-border-radius:3px;border-radius:3px}#comment_quicktags{text-align:left;padding:10px 0 2px 0;display:block}#comment_quicktags input.ed_button{background:#f4f4f4;border:2px solid #cccccc;color:#444444;margin:2px 4px 2px 0;width:auto;padding:0 4px;height:24px;line-height:16px}#comment_quicktags input.ed_button_hover{background:#dddddd;border:2px solid #666666;color:#000000;margin:2px 4px 2px 0;width:auto;padding:0 4px;height:24px;line-height:16px;cursor:pointer}#comment_quicktags #ed_strong{font-weight:bold}#comment_quicktags #ed_em{font-style:italic}@media print{body{background:white;color:black;margin:0;font-size:10pt !important;font-family:arial,sans-serif;}div.post-footer{line-height:normal !important;color:#555 !important;font-size:9pt !important}a:link,a:visited,a:active,a:hover{text-decoration:underline !important;color:#000}h2{color:#000;font-size:14pt !important;font-weight:normal !important}h3{color:#000;font-size:12pt !important;}#header,#footer,.colone,.colone-inner,.colthree-inner,.colthree,.navigation,.navigation-top,.navigation-middle,.navigation-bottom,.wp-pagenavi-navigation,#comment,#respond,.remove-for-print{display:none}td#left,td#right,td#left-inner,td#right-inner{width:0;display:none}td#middle{width:100% !important;display:block}*:lang(en) td#left{ display:none}*:lang(en) td#right{ display:none}*:lang(en) td#left-inner{ display:none}*:lang(en) td#right-inner{ display:none}td#left:empty{ display:none}td#right:empty{ display:none}td#left-inner:empty{ display:none}td#right-inner:empty{ display:none}}ul.rMenu,ul.rMenu ul,ul.rMenu li,ul.rMenu a{display:block;margin:0;padding:0}ul.rMenu,ul.rMenu li,ul.rMenu ul{list-style:none}ul.rMenu ul{display:none}ul.rMenu li{position:relative;z-index:1}ul.rMenu li:hover{z-index:999}ul.rMenu li:hover > ul{display:block;position:absolute}ul.rMenu li:hover{background-position:0 0} ul.rMenu-hor li{float:left;width:auto}ul.rMenu-hRight li{float:right}ul.sub-menu li,ul.rMenu-ver li{float:none}div#menu1 ul.sub-menu,div#menu1 ul.sub-menu ul,div#menu1 ul.rMenu-ver,div#menu1 ul.rMenu-ver ul{width:11em}div#menu2 ul.sub-menu,div#menu2 ul.sub-menu ul,div#menu2 ul.rMenu-ver,div#menu2 ul.rMenu-ver ul{width:11em}ul.rMenu-wide{width:100%}ul.rMenu-vRight{float:right}ul.rMenu-lFloat{float:left}ul.rMenu-noFloat{float:none}div.rMenu-center ul.rMenu{float:left;position:relative;left:50%}div.rMenu-center ul.rMenu li{position:relative;left:-50%}div.rMenu-center ul.rMenu li li{left:auto}ul.rMenu-hor ul{top:auto;right:auto;left:auto;margin-top:-1px}ul.rMenu-hor ul ul{margin-top:0;margin-left:0px}ul.sub-menu ul,ul.rMenu-ver ul{left:100%;right:auto;top:auto;top:0}ul.rMenu-vRight ul,ul.rMenu-hRight ul.sub-menu ul,ul.rMenu-hRight ul.rMenu-ver ul{left:-100%;right:auto;top:auto}ul.rMenu-hRight ul{left:auto;right:0;top:auto;margin-top:-1px}div#menu1 ul.rMenu{background:#ffffff;border:dashed 1px #cccccc}div#menu2 ul.rMenu{background:#777777;border:solid 1px #000000}div#menu1 ul.rMenu li a{border:dashed 1px #cccccc}div#menu2 ul.rMenu li a{border:solid 1px #000000}ul.rMenu-hor li{margin-bottom:-1px;margin-top:-1px;margin-left:-1px}ul#rmenu li{}ul#rmenu li ul li{}ul.rMenu-hor{padding-left:1px }ul.sub-menu li,ul.rMenu-ver li{margin-left:0;margin-top:-1px; }div#menu1 ul.sub-menu,div#menu1 ul.rMenu-ver{border-top:dashed 1px #cccccc}div#menu2 ul.sub-menu,div#menu2 ul.rMenu-ver{border-top:solid 1px #000000}div#menu1 ul.rMenu li a{padding:4px 5px}div#menu2 ul.rMenu li a{padding:4px 5px}div#menu1 ul.rMenu li a:link,div#menu1 ul.rMenu li a:hover,div#menu1 ul.rMenu li a:visited,div#menu1 ul.rMenu li a:active{text-decoration:none;margin:0;color:#777777;text-transform:uppercase;font:11px Arial,Verdana,sans-serif;}div#menu2 ul.rMenu li a:link,div#menu2 ul.rMenu li a:hover,div#menu2 ul.rMenu li a:visited,div#menu2 ul.rMenu li a:active{text-decoration:none;margin:0;color:#ffffff;text-transform:uppercase;font:11px Arial,Verdana,sans-serif;}div#menu1 ul.rMenu li{background-color:#ffffff}div#menu2 ul.rMenu li{background-color:#777777}div#menu1 ul.rMenu li:hover,div#menu1 ul.rMenu li.sfhover{background:#dddddd}div#menu2 ul.rMenu li:hover,div#menu2 ul.rMenu li.sfhover{background:#000000}div#menu1 ul.rMenu li.current-menu-item > a:link,div#menu1 ul.rMenu li.current-menu-item > a:active,div#menu1 ul.rMenu li.current-menu-item > a:hover,div#menu1 ul.rMenu li.current-menu-item > a:visited,div#menu1 ul.rMenu li.current_page_item > a:link,div#menu1 ul.rMenu li.current_page_item > a:active,div#menu1 ul.rMenu li.current_page_item > a:hover,div#menu1 ul.rMenu li.current_page_item > a:visited{background-color:#eeeeee;color:#000000}div#menu1 ul.rMenu li.current-menu-item a:link,div#menu1 ul.rMenu li.current-menu-item a:active,div#menu1 ul.rMenu li.current-menu-item a:hover,div#menu1 ul.rMenu li.current-menu-item a:visited,div#menu1 ul.rMenu li.current_page_item a:link,div#menu1 ul.rMenu li.current_page_item a:active,div#menu1 ul.rMenu li.current_page_item a:hover,div#menu1 ul.rMenu li.current_page_item a:visited,div#menu1 ul.rMenu li a:hover{background-color:#eeeeee;color:#000000}div#menu2 ul.rMenu li.current-menu-item > a:link,div#menu2 ul.rMenu li.current-menu-item > a:active,div#menu2 ul.rMenu li.current-menu-item > a:hover,div#menu2 ul.rMenu li.current-menu-item > a:visited,div#menu2 ul.rMenu li.current-cat > a:link,div#menu2 ul.rMenu li.current-cat > a:active,div#menu2 ul.rMenu li.current-cat > a:hover,div#menu2 ul.rMenu li.current-cat > a:visited{background-color:#cc0000;color:#ffffff}div#menu2 ul.rMenu li.current-menu-item a:link,div#menu2 ul.rMenu li.current-menu-item a:active,div#menu2 ul.rMenu li.current-menu-item a:hover,div#menu2 ul.rMenu li.current-menu-item a:visited,div#menu2 ul.rMenu li.current-cat a:link,div#menu2 ul.rMenu li.current-cat a:active,div#menu2 ul.rMenu li.current-cat a:hover,div#menu2 ul.rMenu li.current-cat a:visited,div#menu2 ul.rMenu li a:hover{background-color:#cc0000;color:#ffffff}div#menu1 ul.rMenu li.rMenu-expand a,div#menu1 ul.rMenu li.rMenu-expand li.rMenu-expand a,div#menu1 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu1 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu1 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu1 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu1 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu1 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu1 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu1 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a{padding-right:15px;padding-left:5px;background-repeat:no-repeat;background-position:100% 50%;background-image:url(http://deeplearning.net/wp-content/themes/atahualpa/images/expand-right.gif)}div#menu2 ul.rMenu li.rMenu-expand a,div#menu2 ul.rMenu li.rMenu-expand li.rMenu-expand a,div#menu2 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu2 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu2 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu2 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu2 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu2 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu2 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,div#menu2 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a{padding-right:15px;padding-left:5px;background-repeat:no-repeat;background-position:100% 50%;background-image:url(http://deeplearning.net/wp-content/themes/atahualpa/images/expand-right-white.gif)}ul.rMenu-vRight li.rMenu-expand a,ul.rMenu-vRight li.rMenu-expand li.rMenu-expand a,ul.rMenu-vRight li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-vRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-vRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-vRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-vRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-vRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-vRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-vRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-hRight li.rMenu-expand a,ul.rMenu-hRight li.rMenu-expand li.rMenu-expand a,ul.rMenu-hRight li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-hRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-hRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-hRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-hRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-hRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-hRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a,ul.rMenu-hRight li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand a{padding-right:5px;padding-left:20px;background-image:url(http://deeplearning.net/wp-content/themes/atahualpa/images/expand-left.gif);background-repeat:no-repeat;background-position:-5px 50%}div#menu1 ul.rMenu-hor li.rMenu-expand a{padding-left:5px;padding-right:15px !important;background-position:100% 50%;background-image:url(http://deeplearning.net/wp-content/themes/atahualpa/images/expand-down.gif)}div#menu2 ul.rMenu-hor li.rMenu-expand a{padding-left:5px;padding-right:15px !important;background-position:100% 50%;background-image:url(http://deeplearning.net/wp-content/themes/atahualpa/images/expand-down-white.gif)}div#menu1 ul.rMenu li.rMenu-expand li a,div#menu1 ul.rMenu li.rMenu-expand li.rMenu-expand li a,div#menu1 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li a,div#menu1 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li a,div#menu1 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li a{background-image:none;padding-right:5px;padding-left:5px}div#menu2 ul.rMenu li.rMenu-expand li a,div#menu2 ul.rMenu li.rMenu-expand li.rMenu-expand li a,div#menu2 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li a,div#menu2 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li a,div#menu2 ul.rMenu li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li.rMenu-expand li a{background-image:none;padding-right:5px;padding-left:5px}* html ul.rMenu{display:inline-block;display:block;position:relative;position:static}* html ul.rMenu ul{float:left;float:none}ul.rMenu ul{background-color:#fff}* html ul.sub-menu li,* html ul.rMenu-ver li,* html ul.rMenu-hor li ul.sub-menu li,* html ul.rMenu-hor li ul.rMenu-ver li{width:100%;float:left;clear:left}*:first-child+html ul.sub-menu > li:hover ul,*:first-child+html ul.rMenu-ver > li:hover ul{min-width:0}ul.rMenu li a{position:relative;min-width:0}* html ul.rMenu-hor li{width:6em;width:auto}* html div.rMenu-center{position:relative;z-index:1}html:not([lang*=""]) div.rMenu-center ul.rMenu li a:hover{height:100%}html:not([lang*=""]) div.rMenu-center ul.rMenu li a:hover{height:auto}* html ul.rMenu ul{display:block;position:absolute}* html ul.rMenu ul,* html ul.rMenu-hor ul,* html ul.sub-menu ul,* html ul.rMenu-ver ul,* html ul.rMenu-vRight ul,* html ul.rMenu-hRight ul.sub-menu ul,* html ul.rMenu-hRight ul.rMenu-ver ul,* html ul.rMenu-hRight ul{left:-10000px}* html ul.rMenu li.sfhover{z-index:999}* html ul.rMenu li.sfhover ul{left:auto}* html ul.rMenu li.sfhover ul ul,* html ul.rMenu li.sfhover ul ul ul{display:none}* html ul.rMenu li.sfhover ul,* html ul.rMenu li li.sfhover ul,* html ul.rMenu li li li.sfhover ul{display:block}* html ul.sub-menu li.sfhover ul,* html ul.rMenu-ver li.sfhover ul{left:60%}* html ul.rMenu-vRight li.sfhover ul,* html ul.rMenu-hRight ul.sub-menu li.sfhover ul* html ul.rMenu-hRight ul.rMenu-ver li.sfhover ul{left:-60%}* html ul.rMenu iframe{position:absolute;left:0;top:0;z-index:-1}* html ul.rMenu{margin-left:1px}* html ul.rMenu ul,* html ul.rMenu ul ul,* html ul.rMenu ul ul ul,* html ul.rMenu ul ul ul ul{margin-left:0}.clearfix:after{ content:".";display:block;height:0;clear:both;visibility:hidden}.clearfix{min-width:0;display:inline-block;display:block}* html .clearfix{height:1%;}.clearboth{clear:both;height:1%;font-size:1%;line-height:1%;display:block;padding:0;margin:0}h1{font-size:34px;line-height:1.2;margin:0.3em 0 10px;}h2{font-size:28px;line-height:1.3;margin:1em 0 .2em;}h3{font-size:24px;line-height:1.3;margin:1em 0 .2em;}h4{font-size:19px;margin:1.33em 0 .2em;}h5{font-size:1.3em;margin:1.67em 0;font-weight:bold;}h6{font-size:1.15em;margin:1.67em 0;font-weight:bold;}</style>
<script type="text/javascript">
//<![CDATA[



/* JQUERY */
jQuery(document).ready(function(){ 
    
   
  

	/* jQuery('ul#rmenu').superfish(); */
	/* jQuery('ul#rmenu').superfish().find('ul').bgIframe({opacity:false}); */
 
	/* For IE6 */
	if (jQuery.browser.msie && /MSIE 6\.0/i.test(window.navigator.userAgent) && !/MSIE 7\.0/i.test(window.navigator.userAgent) && !/MSIE 8\.0/i.test(window.navigator.userAgent)) {

		/* Max-width for images in IE6 */		
		var centerwidth = jQuery("td#middle").width(); 
		
		/* Images without caption */
		jQuery(".post img").each(function() { 
			var maxwidth = centerwidth - 10 + 'px';
			var imgwidth = jQuery(this).width(); 
			var imgheight = jQuery(this).height(); 
			var newimgheight = (centerwidth / imgwidth * imgheight) + 'px';	
			if (imgwidth > centerwidth) { 
				jQuery(this).css({width: maxwidth}); 
				jQuery(this).css({height: newimgheight}); 
			}
		});
		
		/* Images with caption */
		jQuery("div.wp-caption").each(function() { 
			var captionwidth = jQuery(this).width(); 
			var maxcaptionwidth = centerwidth + 'px';
			var captionheight = jQuery(this).height();
			var captionimgwidth =  jQuery("div.wp-caption img").width();
			var captionimgheight =  jQuery("div.wp-caption img").height();
			if (captionwidth > centerwidth) { 
				jQuery(this).css({width: maxcaptionwidth}); 
				var newcaptionheight = (centerwidth / captionwidth * captionheight) + 'px';
				var newcaptionimgheight = (centerwidth / captionimgwidth * captionimgheight) + 'px';
				jQuery(this).css({height: newcaptionheight}); 
				jQuery("div.wp-caption img").css({height: newcaptionimgheight}); 
				}
		});
		
		/* sfhover for LI:HOVER support in IE6: */
		jQuery("ul li").
			hover( function() {
					jQuery(this).addClass("sfhover")
				}, 
				function() {
					jQuery(this).removeClass("sfhover")
				} 
			); 

	/* End IE6 */
	}
	
	
	
	/* Since 3.7.8: Auto resize videos (embed and iframe elements) 
	TODO: Parse parent's dimensions only once per layout column, not per video
	*/
	function bfa_resize_video() {
		jQuery('embed, iframe').each( function() {
			var video = jQuery(this),
			videoWidth = video.attr('width'); // use the attr here, not width() or css()
			videoParent = video.parent(),
			videoParentWidth = parseFloat( videoParent.css( 'width' ) ),
			videoParentBorder = parseFloat( videoParent.css( 'border-left-width' ) ) 
										+  parseFloat( videoParent.css( 'border-right-width' ) ),
			videoParentPadding = parseFloat( videoParent.css( 'padding-left' ) ) 
										+  parseFloat( videoParent.css( 'padding-right' ) ),
			maxWidth = videoParentWidth - videoParentBorder - videoParentPadding;

			if( videoWidth > maxWidth ) {
				var videoHeight = video.attr('height'),
				videoMaxHeight = ( maxWidth / videoWidth * videoHeight );
				video.attr({ width: maxWidth, height: videoMaxHeight });
			} 

		});	
	}
	bfa_resize_video();
	jQuery(window).resize( bfa_resize_video );

		
	jQuery(".post table tr").
		mouseover(function() {
			jQuery(this).addClass("over");
		}).
		mouseout(function() {
			jQuery(this).removeClass("over");
		});

	
	jQuery(".post table tr:even").
		addClass("alt");

	
	jQuery("input.text, input.TextField, input.file, input.password, textarea").
		focus(function () {  
			jQuery(this).addClass("highlight"); 
		}).
		blur(function () { 
			jQuery(this).removeClass("highlight"); 
		})
	
	jQuery("input.inputblur").
		focus(function () {  
			jQuery(this).addClass("inputfocus"); 
		}).
		blur(function () { 
			jQuery(this).removeClass("inputfocus"); 
		})

		

	
	jQuery("input.button, input.Button, input#submit").
		mouseover(function() {
			jQuery(this).addClass("buttonhover");
		}).
		mouseout(function() {
			jQuery(this).removeClass("buttonhover");
		});

	/* toggle "you can use these xhtml tags" */
	jQuery("a.xhtmltags").
		click(function(){ 
			jQuery("div.xhtml-tags").slideToggle(300); 
		});

	/* For the Tabbed Widgets plugin: */
	jQuery("ul.tw-nav-list").
		addClass("clearfix");

		
	
});

//]]>
</script>

<!--[if IE 6]>
<script type="text/javascript">DD_roundies.addRule("a.posts-icon, a.comments-icon, a.email-icon, img.logo");</script>
<![endif]-->
<style>.ios7.web-app-mode.has-fixed header{ background-color: rgba(3,122,221,.88);}</style></head>
<body class="page-template-default page page-id-157" >

<div id="wrapper">
<div id="container">
<table id="layout" border="0" cellspacing="0" cellpadding="0">
<colgroup>
<col class="colone" /><col class="coltwo" />
<col class="colthree" /></colgroup> 


	<tr>

		<!-- Header -->
		<td id="header" colspan="3">

		<div id="menu1"><ul id="rmenu2" class="clearfix rMenu-hor rMenu">
<li class="page_item"><a href="http://deeplearning.net/" title="Deep Learning"><span>Home</span></a></li>
<li class="page_item page-item-2"><a href="http://deeplearning.net/"><span>About</span></a></li>
<li class="rMenu-expand page_item page-item-4 page_item_has_children"><a href="http://deeplearning.net/reading-list/"><span>Reading List</span></a>
 <ul class="rMenu-ver">
	<li class="page_item page-item-7"><a href="http://deeplearning.net/reading-list/tutorials/"><span>Tutorials</span></a></li>
</ul>
</li>
<li class="page_item page-item-12"><a href="http://deeplearning.net/software_links/"><span>Software links</span></a></li>
<li class="page_item page-item-13"><a href="http://deeplearning.net/blog/"><span>Blog</span></a></li>
<li class="page_item page-item-29"><a href="http://deeplearning.net/demos/"><span>Demos</span></a></li>
<li class="page_item page-item-31"><a href="http://deeplearning.net/datasets/"><span>Datasets</span></a></li>
<li class="page_item page-item-132"><a href="http://deeplearning.net/events/"><span>Events</span></a></li>
<li class="page_item page-item-157 current_page_item"><a href="http://deeplearning.net/bibliography/"><span>Bibliography</span></a></li>
<li class="page_item page-item-216"><a href="http://deeplearning.net/deep-learning-research-groups-and-labs/"><span>Deep Learning Research Groups</span></a></li>
<li class="rMenu-expand page_item page-item-4789 page_item_has_children"><a href="http://deeplearning.net/icml2013-workshop-competition/"><span>ICML 2013 Challenges in Representation Learning</span></a>
 <ul class="rMenu-ver">
	<li class="page_item page-item-7113"><a href="http://deeplearning.net/icml2013-workshop-competition/challenges/"><span>Challenges</span></a></li>
	<li class="page_item page-item-7428"><a href="http://deeplearning.net/icml2013-workshop-competition/schedule/"><span>Schedule</span></a></li>
</ul>
</li>
<li class="page_item page-item-5535"><a href="http://deeplearning.net/deep-learning-job-listings/"><span>Deep Learning Job Listings</span></a></li>
<li class="page_item page-item-300589"><a href="http://deeplearning.net/startup-news/"><span>Startup News</span></a></li>
</ul></div>
 <table id="logoarea" cellpadding="0" cellspacing="0" border="0" width="100%"><tr><td rowspan="2" valign="middle" class="logoarea-logo"><a href="http://deeplearning.net/"><img class="logo" src="http://deeplearning.net/wp-content/ata-images/Deep_learning.png" alt="Deep Learning" /></a></td><td rowspan="2" valign="middle" class="logoarea-title"><h2 class="blogtitle"><a href="http://deeplearning.net/">Deep Learning</a></h2><p class="tagline">&#8230; moving beyond shallow machine learning since 2006!</p></td><td class="feed-icons" valign="middle" align="right"><div class="clearfix rss-box"><a class="comments-icon" href="http://deeplearning.net/comments/feed/" title="Subscribe to the COMMENTS feed">Comments</a><a class="posts-icon" href="http://deeplearning.net/feed/" title="Subscribe to the POSTS feed">Posts</a></div></td></tr><tr><td valign="bottom" class="search-box" align="right"><div class="searchbox">
					<form method="get" class="searchform" action="http://deeplearning.net/">
					<div class="searchbox-form"><input type="text" class="text inputblur" onfocus="this.value=''" 
						value="" onblur="this.value=''" name="s" /></div>
					</form>
				</div>
				</td></tr></table> <div class="horbar1">&nbsp;</div> <div id="imagecontainer-pre" class="header-image-container-pre">    <div id="imagecontainer" class="header-image-container" style="background: url('http://deeplearning.net/wp-content/themes/atahualpa/images/header/neurons.jpg') top center no-repeat;"></div><div class="codeoverlay"></div><div class="opacityleft">&nbsp;</div><div class="opacityright">&nbsp;</div></div> <div class="horbar2">&nbsp;</div>
		</td>
		<!-- / Header -->

	</tr>
	<!-- Main Body -->	
	<tr id="bodyrow">

				<!-- Left Sidebar -->
		<td id="left">

			<div id="archives-3" class="widget widget_archive"><div class="widget-title"><h3>Archives</h3></div>		<ul>
			<li><a href='http://deeplearning.net/2016/07/'>July 2016</a></li>
	<li><a href='http://deeplearning.net/2015/12/'>December 2015</a></li>
	<li><a href='http://deeplearning.net/2015/11/'>November 2015</a></li>
	<li><a href='http://deeplearning.net/2015/10/'>October 2015</a></li>
	<li><a href='http://deeplearning.net/2015/09/'>September 2015</a></li>
	<li><a href='http://deeplearning.net/2015/07/'>July 2015</a></li>
	<li><a href='http://deeplearning.net/2014/11/'>November 2014</a></li>
	<li><a href='http://deeplearning.net/2014/10/'>October 2014</a></li>
	<li><a href='http://deeplearning.net/2014/09/'>September 2014</a></li>
	<li><a href='http://deeplearning.net/2014/05/'>May 2014</a></li>
	<li><a href='http://deeplearning.net/2014/04/'>April 2014</a></li>
	<li><a href='http://deeplearning.net/2014/01/'>January 2014</a></li>
	<li><a href='http://deeplearning.net/2013/12/'>December 2013</a></li>
	<li><a href='http://deeplearning.net/2013/10/'>October 2013</a></li>
	<li><a href='http://deeplearning.net/2013/09/'>September 2013</a></li>
	<li><a href='http://deeplearning.net/2013/08/'>August 2013</a></li>
	<li><a href='http://deeplearning.net/2013/07/'>July 2013</a></li>
	<li><a href='http://deeplearning.net/2013/06/'>June 2013</a></li>
	<li><a href='http://deeplearning.net/2013/05/'>May 2013</a></li>
	<li><a href='http://deeplearning.net/2013/04/'>April 2013</a></li>
	<li><a href='http://deeplearning.net/2013/03/'>March 2013</a></li>
	<li><a href='http://deeplearning.net/2013/02/'>February 2013</a></li>
	<li><a href='http://deeplearning.net/2013/01/'>January 2013</a></li>
	<li><a href='http://deeplearning.net/2012/12/'>December 2012</a></li>
	<li><a href='http://deeplearning.net/2012/01/'>January 2012</a></li>
	<li><a href='http://deeplearning.net/2011/08/'>August 2011</a></li>
	<li><a href='http://deeplearning.net/2011/06/'>June 2011</a></li>
	<li><a href='http://deeplearning.net/2011/02/'>February 2011</a></li>
	<li><a href='http://deeplearning.net/2010/12/'>December 2010</a></li>
	<li><a href='http://deeplearning.net/2010/10/'>October 2010</a></li>
	<li><a href='http://deeplearning.net/2010/04/'>April 2010</a></li>
	<li><a href='http://deeplearning.net/2010/03/'>March 2010</a></li>
	<li><a href='http://deeplearning.net/2010/01/'>January 2010</a></li>
		</ul>
		</div><div id="meta-4" class="widget widget_meta"><div class="widget-title"><h3>Meta</h3></div>			<ul>
						<li><a href="http://deeplearning.net/wp-login.php">Log in</a></li>
			<li><a href="http://deeplearning.net/feed/">Entries <abbr title="Really Simple Syndication">RSS</abbr></a></li>
			<li><a href="http://deeplearning.net/comments/feed/">Comments <abbr title="Really Simple Syndication">RSS</abbr></a></li>
			<li><a href="https://wordpress.org/" title="Powered by WordPress, state-of-the-art semantic personal publishing platform.">WordPress.org</a></li>			</ul>
			</div>
		</td>
		<!-- / Left Sidebar -->
		
				

		<!-- Main Column -->
		<td id="middle">

    
		
		
								<div class="post post-157 page type-page status-publish hentry odd" id="post-157">
						<div class="post-headline">		<h1>Bibliography</h1>
		</div>				<div class="post-bodycopy clearfix"><p>This is a list of publications, aimed at being a comprehensive bibliography of the field. Should you wish to have your publications listed here, you can either <a href="mailto:admin@deeplearning.net">email</a> us your BibTex .bib file or a link to your up-to-date .bib file (the plugin will automatically update the list below using the bibtex entries from the link provided). See <a href="http://wordpress.org/extend/plugins/bib2html/other_notes/">this help page</a> for instructions on obtaining such a link with services like citeulike or bibsonomy.</p>
<p>If you&#8217;re registered on this blog and have editor access, you can edit this page and add the link yourself.</p>
<p>Also we recommend you to have a look at Memkite&#8217;s deep learning reading list as well:</p>
<p><a href="http://memkite.com/deep-learning-bibliography/">http://memkite.com/deep-learning-bibliography/</a></p>
<p>University of Montreal&#8217;s LISA lab deep learning publications:</p>
<ul>
</p>
<li>
<div>[2010,article] Y. Bengio, O. Delalleau, and C. Simard, &quot;Decision Trees do not Generalize to New Variations,&quot; <em>Computational Intelligence</em>, vol. 26, iss. 4, pp. 449-467, 2010.</div>
<p> <a href=#Bengio-decision-trees10 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Bengio-decision-trees10>
         <code>@ARTICLE{Bengio-decision-trees10, <br />
 &nbsp;&nbsp;author = {Bengio, Yoshua and Delalleau, Olivier and Simard, Clarence}, <br />
 &nbsp; keywords = {curse of dimensionality, decision trees, parity function}, <br />
 &nbsp; month = nov, title = {Decision Trees do not Generalize to New Variations}, <br />
 &nbsp; journal = {Computational Intelligence}, <br />
 &nbsp; volume = {26}, <br />
 &nbsp; number = {4}, <br />
 &nbsp; year = {2010}, <br />
 &nbsp; pages = {449--467}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2010,inproceedings] D. Erhan, A. Courville, Y. Bengio, and P. Vincent, &quot;Why Does Unsupervised Pre-training Help Deep Learning?,&quot; in <em>Proceedings of AISTATS 2010</em>,  2010, pp. 201-208.</div>
<p> <a href=#Erhan-aistats-2010 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Erhan-aistats-2010>
         <code>@INPROCEEDINGS{Erhan-aistats-2010, <br />
 &nbsp;&nbsp;author = {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal}, <br />
 &nbsp; month = may, title = {Why Does Unsupervised Pre-training Help Deep Learning?}, <br />
 &nbsp; booktitle = {Proceedings of AISTATS 2010}, <br />
 &nbsp; volume = {9}, <br />
 &nbsp; year = {2010}, <br />
 &nbsp; pages = {201-208}, <br />
 &nbsp; location = {Chia Laguna Resort, Sardinia, Italy}, <br />
 &nbsp; abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants with impressive results being obtained in several areas, mostly on vision and language datasets. The best results obtained on supervised learning tasks often involve an unsupervised learning component, usually in an unsupervised pre-training phase. The main question investigated here is the following: why does unsupervised pre-training work so well? Through extensive experimentation, we explore several possible explanations discussed in the literature including its action as a regularizer (Erhan et al. 2009) and as an aid to optimization (Bengio et al. 2007). Our results build on the work of Erhan et al. 2009, showing that unsupervised pre-training appears to play predominantly a regularization role in subsequent supervised training. However our results in an online setting, with a virtually unlimited data stream, point to a somewhat more nuanced interpretation of the roles of optimization and regularization in the unsupervised pre-training effect.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2010,inproceedings] Y. Bengio and X. Glorot, &quot;Understanding the difficulty of training deep feedforward neural networks,&quot; in <em>Proceedings of AISTATS 2010</em>,  2010, pp. 249-256.</div>
<p> <a href=#GlorotAISTATS2010 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=GlorotAISTATS2010>
         <code>@INPROCEEDINGS{GlorotAISTATS2010, <br />
 &nbsp;&nbsp;author = {Bengio, Yoshua and Glorot, Xavier}, <br />
 &nbsp; month = may, title = {Understanding the difficulty of training deep feedforward neural networks}, <br />
 &nbsp; booktitle = {Proceedings of AISTATS 2010}, <br />
 &nbsp; volume = {9}, <br />
 &nbsp; year = {2010}, <br />
 &nbsp; pages = {249-256}, <br />
 &nbsp; location = {Chia Laguna Resort, Sardinia, Italy}, <br />
 &nbsp; abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2010,article] D. Erhan, Y. Bengio, A. Courville, P. Manzagol, P. Vincent, and S. Bengio, &quot;Why Does Unsupervised Pre-training Help Deep Learning?,&quot; , vol. 11, pp. 625-660, 2010.</div>
<p> <a href=#Erhan-al-2010 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Erhan-al-2010>
         <code>@ARTICLE{Erhan+al-2010, <br />
 &nbsp;&nbsp;author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy}, <br />
 &nbsp; month = feb, title = {Why Does Unsupervised Pre-training Help Deep Learning?}, <br />
 &nbsp; volume = {11}, <br />
 &nbsp; year = {2010}, <br />
 &nbsp; pages = {625--660}, <br />
 &nbsp; crossref = {JMLR}, <br />
 &nbsp; abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language datasets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: why does unsupervised pre-training work and why does it work so well? Answering these questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that are better in terms of the underlying data distribution; the evidence from these results supports a regularization explanation for the effect of pre-training.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2009,inproceedings] D. Erhan, P. Manzagol, Y. Bengio, S. Bengio, and P. Vincent, &quot;The Difficulty of Training Deep Architectures and the effect of Unsupervised Pre-Training.&quot;  2009, pp. 153-160.</div>
<p> <a href=#Erhan2009 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Erhan2009>
         <code>@INPROCEEDINGS{Erhan2009, <br />
 &nbsp;&nbsp;author = {Erhan, Dumitru and Manzagol, Pierre-Antoine and Bengio, Yoshua and Bengio, Samy and Vincent, Pascal}, <br />
 &nbsp; keywords = {Deep Networks}, <br />
 &nbsp; month = apr, title = {The Difficulty of Training Deep Architectures and the effect of Unsupervised Pre-Training}, <br />
 &nbsp; year = {2009}, <br />
 &nbsp; pages = {153--160}, <br />
 &nbsp; crossref = {xAISTATS2009}, <br />
 &nbsp; abstract = {Whereas theoretical work suggests that deep architectures might be more efﬁcient at representing highly-varying functions, training deep architectures was unsuccessful until the recent advent of algorithms based on unsupervised pretraining. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difﬁcult learning problem. Answering these questions is important if learning in deep architectures is to be further improved. We attempt to shed some light on these questions through extensive simulations. The experiments conﬁrm and clarify the advantage of unsupervised pre-training. They demonstrate the robustness of the training procedure with respect to the random initialization, the positive effect of pre-training in terms of optimization and its role as a regularizer. We empirically show the inﬂuence of pre-training with respect to architecture depth, model capacity, and number of training examples.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2009,techreport] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, &quot;Curriculum Learning,&quot; Département d&#8217;informatique et recherche opérationnelle, Université de Montréal, 1330, 2009.</div>
<p> <a href=#Bengio-al-2009-TR class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Bengio-al-2009-TR>
         <code>@TECHREPORT{Bengio+al-2009-TR, <br />
 &nbsp;&nbsp;author = {Bengio, Yoshua and Louradour, Jerome and Collobert, Ronan and Weston, Jason}, <br />
 &nbsp; title = {Curriculum Learning}, <br />
 &nbsp; number = {1330}, <br />
 &nbsp; year = {2009}, <br />
 &nbsp; institution = {D{\'{e}}partement d'informatique et recherche op{\'{e}}rationnelle, Universit{\'{e}} de Montr{\'{e}}al}, <br />
 &nbsp; abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them 'curriculum learning'. In the context of recent research studying the difﬁculty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that signiﬁcant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2009,inproceedings] G. Attardi, F. Dell&#8217;Orletta, M. Simi, and J. Turian, &quot;Accurate Dependency Parsing with a Stacked Multilayer Perceptron,&quot; in <em>Proceeding of Evalita 2009</em>,  2009.</div>
<p> <a href=#Attardi-al-2009 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Attardi-al-2009>
         <code>@INPROCEEDINGS{Attardi+al-2009, <br />
 &nbsp;&nbsp;author = {Attardi, Giuseppe and Dell'Orletta, Felice and Simi, Maria and Turian, Joseph}, <br />
 &nbsp; keywords = {classifier, dependency parsing, natural language, parser, perceptron}, <br />
 &nbsp; title = {Accurate Dependency Parsing with a Stacked Multilayer Perceptron}, <br />
 &nbsp; booktitle = {Proceeding of Evalita 2009}, <br />
 &nbsp; series = {LNCS}, <br />
 &nbsp; year = {2009}, <br />
 &nbsp; publisher = {Springer}, <br />
 &nbsp; abstract = {Abstract. DeSR is a statistical transition-based dependency parser which learns from annotated corpora which actions to perform for building parse trees while scanning a sentence. We describe recent improvements to the parser, in particular stacked parsing, exploiting a beam search strategy and using a Multilayer Perceptron classifier. For the Evalita 2009 Dependency Parsing task DesR was configured to use a combination of stacked parsers. The stacked combination achieved the best accuracy scores in both the main and pilot subtasks. The contribution to the result of various choices is analyzed, in particular for taking advantage of the peculiar features of the TUT Treebank.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2009,phdthesis] H. Larochelle, &quot;Étude de techniques d&#8217;apprentissage non-supervisé pour l&#8217;amélioration de l&#8217;entra\^\inement supervisé de modèles connexionnistes,&quot; PhD Thesis , 2009.</div>
<p> <a href=#Larochelle-PhD-2009 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Larochelle-PhD-2009>
         <code>@PHDTHESIS{Larochelle-PhD-2009, <br />
 &nbsp;&nbsp;author = {Larochelle, Hugo}, <br />
 &nbsp; keywords = {apprentissage non-supervis{\'{e}}, <br />
 &nbsp; architecture profonde, autoassociateur, autoencodeur, machine de Boltzmann restreinte, r{\'{e}}seau de neurones artificiel}, <br />
 &nbsp; month = mar, title = {{\'{E}}tude de techniques d'apprentissage non-supervis{\'{e}} pour l'am{\'{e}}lioration de l'entra{\^{\i}}nement supervis{\'{e}} de mod{\`{e}}les connexionnistes}, <br />
 &nbsp; year = {2009}, <br />
 &nbsp; school = {University of Montr{\'{e}}al}, <br />
 &nbsp; abstract = {Le domaine de l'intelligence artificielle a pour objectif le d{\'{e}}veloppement de syst{\`{e}}mes informatiques capables de simuler des comportements normalement associ{\'{e}}s {\`{a}} l'intelligence humaine. On aimerait entre autres pouvoir construire une machine qui puisse r{\'{e}}soudre des t{\^{a}}ches li{\'{e}}es {\`{a}} la vision (e.g., la reconnaissance d'objet), au traitement de la langue (e.g., l'identification du sujet d'un texte) ou au traitement de signaux sonores (e.g., la reconnaissance de la parole). Une approche d{\'{e}}velopp{\'{e}}e afin de r{\'{e}}soudre ce genre de t{\^{a}}ches est bas{\'{e}}e sur l'apprentissage automatique de mod{\`{e}}les {\`{a}} partir de donn{\'{e}}es {\'{e}}tiquet{\'{e}}es refl{\'{e}}tant le comportement intelligent {\`{a}} {\'{e}}muler. Entre autre, il a {\'{e}}t{\'{e}} propos{\'{e}} de mod{\'{e}}liser le calcul n{\'{e}}cessaire {\`{a}} la r{\'{e}}solution d'une t{\^{a}}che {\`{a}} l'aide d'un r{\'{e}}seau de neurones artificiel, dont il est possible d'adapter le comportement {\`{a}} l'aide de la r{\'{e}}tropropagation [99, 131] d'un gradient informatif sur les erreurs commises par le r{\'{e}}seau. Populaire durant les ann{\'{e}}es 80, cette approche sp{\'{e}}cifique a depuis perdu partiellement de son attrait, suite au d{\'{e}}veloppement des m{\'{e}}thodes {\`{a}} noyau. Celles-ci sont souvent plus stables, plus faciles {\`{a}} utiliser et leur performance est souvent au moins aussi {\'{e}}lev{\'{e}}e pour une vaste gamme de probl{\`{e}}mes. Les m{\'{e}}thodes d'apprentissage automatique ont donc progress{\'{e}} dans leur fonctionnement, mais aussi dans la complexit{\'{e}} des probl{\`{e}}mes auxquels elles se sont attaqu{\'{e}}. Ainsi, plus r{\'{e}}cemment, des travaux [12, 15] ont commenc{\'{e}} {\`{a}} {\'{e}}mettre des doutes sur la capacit{\'{e}} des machines {\`{a}} noyau {\`{a}} pouvoir efficacement r{\'{e}}soudre des probl{\`{e}}mes de la complexit{\'{e}} requise par l'intelligence artificielle. Parall{\`{e}}lement, Hinton et al. [81] faisaient une perc{\'{e}}e dans l'apprentissage automatique de r{\'{e}}seaux de neurones, en proposant une proc{\'{e}}dure permettant l'entra{\^{\i}}nement de r{\'{e}}seaux de neurones d'une plus grande complexit{\'{e}} (i.e., avec plus de couches de neurones cach{\'{e}}es) qu'il n'{\'{e}}tait possible auparavant. C'est dans ce contexte qu'ont {\'{e}}t{\'{e}} conduits les travaux de cette th{\`{e}}se. Cette th{\`{e}}se d{\'{e}}bute par une exposition des principes de base de l'apprentissage automatique (chapitre 1) et une discussion des obstacles {\`{a}} l'obtention d'un mod{\`{e}}le ayant une bonne performance de g{\'{e}}n{\'{e}}ralisation (chapitre 2). Puis, sont pr{\'{e}}sent{\'{e}}es les contributions apport{\'{e}}es dans le cadre de cinq articles, contributions qui sont toutes bas{\'{e}}es sur l'utilisation d'une certaine forme d'apprentissage non-supervis{\'{e}}. Le premier article (chapitre 4) propose une m{\'{e}}thode d'entra{\^{\i}}nement pour un type sp{\'{e}}cifique de r{\'{e}}seau {\`{a}} une seule couche cach{\'{e}}e (la machine de Boltzmann restreinte) bas{\'{e}}e sur une combinaison des apprentissages supervis{\'{e}} et non-supervis{\'{e}}. Cette m{\'{e}}thode permet d'obtenir une meilleure performance de g{\'{e}}n{\'{e}}ralisation qu'un r{\'{e}}seau de neurones standard ou qu'une machine {\`{a}} vecteurs de support {\`{a}} noyau, et met en {\'{e}}vidence de fa{\c c}on explicite les b{\'{e}}n{\'{e}}fices qu'apporte l'apprentissage non-supervis{\'{e}} {\`{a}} l'entra{\^{\i}}nement d'un r{\'{e}}seau de neurones. Ensuite, dans le second article (chapitre 6), on {\'{e}}tudie et {\'{e}}tend la proc{\'{e}}dure d'entra{\^{\i}}nement propos{\'{e}}e par Hinton et al. [81]. Plus sp{\'{e}}cifiquement, on y propose une approche diff{\'{e}}rente mais plus flexible pour initialiser un r{\'{e}}seau {\`{a}} plusieurs couches cach{\'{e}}es, bas{\'{e}}e sur un r{\'{e}}seau autoassociateur. On y explore aussi l'impact du nombre de couches et de neurones par couche sur la performance d'un r{\'{e}}seau et on y d{\'{e}}crit diff{\'{e}}rentes variantes mieux adapt{\'{e}}es {\`{a}} l'apprentissage en ligne ou pour donn{\'{e}}es {\`{a}} valeurs continues. Dans le troisi{\`{e}}me article (chapitre 8), on explore plut{\^{o}}t la performance de r{\'{e}}seaux profonds sur plusieurs probl{\`{e}}mes de classification diff{\'{e}}rents. Les probl{\`{e}}mes choisis ont la propri{\'{e}}t{\'{e}} d'avoir {\'{e}}t{\'{e}} g{\'{e}}n{\'{e}}r{\'{e}}s {\`{a}} partir de plusieurs facteurs de variation. Cette propri{\'{e}}t{\'{e}}, <br />
 &nbsp; qui caract{\'{e}}rise les probl{\`{e}}mes li{\'{e}}s {\`{a}} l'intelligence artificielle, pose difficult{\'{e}} aux machines {\`{a}} noyau, tel que confirm{\'{e}} par les exp{\'{e}}riences de cet article. Le quatri{\`{e}}me article (chapitre 10) pr{\'{e}}sente une am{\'{e}}lioration de l'approche bas{\'{e}}e sur les r{\'{e}}seaux autoassociateurs. Cette am{\'{e}}lioration applique une modification simple {\`{a}} la proc{\'{e}}dure d'entra{\^{\i}}nement d'un r{\'{e}}seau autoassociateur, en « bruitant » les entr{\'{e}}es du r{\'{e}}seau afin que celui-ci soit forc{\'{e}} {\`{a}} la d{\'{e}}bruiter. Le cinqui{\`{e}}me et dernier article (chapitre 12) apporte une autre am{\'{e}}lioration aux r{\'{e}}seaux autoassociateurs, en permettant des interactions d'inhibition ou d'excitation entre les neurones cach{\'{e}}s de ces r{\'{e}}seaux. On y d{\'{e}}montre que de telles interactions peuvent {\^{e}}tre apprises et sont b{\'{e}}n{\'{e}}fiques {\`{a}} la performance d'un r{\'{e}}seau profond.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2009,article] H. Larochelle, Y. Bengio, J. Louradour, and P. Lamblin, &quot;Exploring Strategies for Training Deep Neural Networks,&quot; , vol. 10, pp. 1-40, 2009.</div>
<p> <a href=#Larochelle-jmlr-2009 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Larochelle-jmlr-2009>
         <code>@ARTICLE{Larochelle-jmlr-2009, <br />
 &nbsp;&nbsp;author = {Larochelle, Hugo and Bengio, Yoshua and Louradour, Jerome and Lamblin, Pascal}, <br />
 &nbsp; month = jan, title = {Exploring Strategies for Training Deep Neural Networks}, <br />
 &nbsp; volume = {10}, <br />
 &nbsp; year = {2009}, <br />
 &nbsp; pages = {1--40}, <br />
 &nbsp; crossref = {JMLR}, <br />
 &nbsp; abstract = {Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2009,article] Y. Bengio and O. Delalleau, &quot;Justifying and Generalizing Contrastive Divergence,&quot; <em>Neural Computation</em>, vol. 21, iss. 6, pp. 1601-1621, 2009.</div>
<p> <a href=#Bengio-Delalleau-2009 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Bengio-Delalleau-2009>
         <code>@ARTICLE{Bengio+Delalleau-2009, <br />
 &nbsp;&nbsp;author = {Bengio, Yoshua and Delalleau, Olivier}, <br />
 &nbsp; month = jun, title = {Justifying and Generalizing Contrastive Divergence}, <br />
 &nbsp; journal = {Neural Computation}, <br />
 &nbsp; volume = {21}, <br />
 &nbsp; number = {6}, <br />
 &nbsp; year = {2009}, <br />
 &nbsp; pages = {1601--1621}, <br />
 &nbsp; abstract = {We study an expansion of the log-likelihood in undirected graphical models such as the Restricted Boltzmann Machine (RBM), where each term in the expansion is associated with a sample in a Gibbs chain alternating between two random variables (the visible vector and the hidden vector, in RBMs). We are particularly interested in estimators of the gradient of the log-likelihood obtained through this expansion. We show that its residual term converges to zero, justifying the use of a truncation, i.e. running only a short Gibbs chain, which is the main idea behind the Contrastive Divergence (CD) estimator of the log-likelihood gradient. By truncating even more, we obtain a stochastic reconstruction error, related through a mean-field approximation to the reconstruction error often used to train autoassociators and stacked auto-associators. The derivation is not specific to the particular parametric forms used in RBMs, and only requires convergence of the Gibbs chain. We present theoretical and empirical evidence linking the number of Gibbs steps $k$ and the magnitude of the RBM parameters to the bias in the CD estimator. These experiments also suggest that the sign of the CD estimator is correct most of the time, even when the bias is large, so that CD-$k$ is a good descent direction even for small $k$.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2009,inproceedings] J. Turian, J. Bergstra, and Y. Bengio, &quot;Quadratic Features and Deep Architectures for Chunking,&quot; in <em>North American Chapter of the Association for Computational Linguistics &#8211; Human Language Technologies (NAACL HLT)</em>, Boulder, Colorado,  2009, pp. 245-248.</div>
<p> <a href=#Turian-al-2009 class="toggle">bibtex</a>   <a href='http://www.aclweb.org/anthology/N/N09/N09-2062' title='Go to document'><img src='http://deeplearning.net/wp-content/plugins/bib2html/external.png' width='10' height='10' alt='Go to document' /></a></p>
<div class="bibtex" id=Turian-al-2009>
         <code>@INPROCEEDINGS{Turian+al-2009, <br />
 &nbsp;&nbsp;author = {Turian, Joseph and Bergstra, James and Bengio, Yoshua}, <br />
 &nbsp; month = jun, title = {Quadratic Features and Deep Architectures for Chunking}, <br />
 &nbsp; booktitle = {North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT)}, <br />
 &nbsp; year = {2009}, <br />
 &nbsp; pages = {245--248}, <br />
 &nbsp; publisher = {Association for Computational Linguistics}, <br />
 &nbsp; address = {Boulder, Colorado}, <br />
 &nbsp; url = {http://www.aclweb.org/anthology/N/N09/N09-2062}, <br />
 &nbsp; abstract = {We experiment with several chunking models. Deeper architectures achieve better generalization. Quadratic filters, a simplification of theoretical model of V1 complex cells, reliably increase accuracy. In fact, logistic regression with quadratic filters outperforms a standard single hidden layer neural network. Adding quadratic filters to logistic regression is almost as effective as feature engineering. Despite predicting each output label independently, our model is competitive with ones that use previous decisions.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2009,inproceedings] &quot;Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS 2009),&quot; in <em>Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS 2009)</em>,  2009.</div>
<p> <a href=#xAISTATS2009 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=xAISTATS2009>
         <code>@INPROCEEDINGS{xAISTATS2009, title = {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS 2009)}, <br />
 &nbsp; booktitle = {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS 2009)}, <br />
 &nbsp; year = {2009}, <br />
 &nbsp; location = {Clearwater (Florida), USA}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2009,article] Y. Bengio, &quot;Learning deep architectures for AI,&quot; <em>Foundations and Trends in Machine Learning</em>, vol. 2, iss. 1, pp. 1-127, 2009.</div>
<p> <a href=#Bengio-2009 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Bengio-2009>
         <code>@ARTICLE{Bengio-2009, <br />
 &nbsp;&nbsp;author = {Bengio, Yoshua}, <br />
 &nbsp; title = {Learning deep architectures for {AI}}, <br />
 &nbsp; journal = {Foundations and Trends in Machine Learning}, <br />
 &nbsp; volume = {2}, <br />
 &nbsp; number = {1}, <br />
 &nbsp; year = {2009}, <br />
 &nbsp; pages = {1--127}, <br />
 &nbsp; note = {Also published as a book. Now Publishers, 2009.}, <br />
 &nbsp; doi = {10.1561/2200000006}, <br />
 &nbsp; abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need {\insist deep architectures}. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2009,techreport] D. Erhan, Y. Bengio, A. Courville, and P. Vincent, &quot;Visualizing Higher-Layer Features of a Deep Network,&quot; University of Montreal, 1341, 2009.</div>
<p> <a href=#visualization_techreport class="toggle">bibtex</a>  </p>
<div class="bibtex" id=visualization_techreport>
         <code>@TECHREPORT{visualization_techreport, <br />
 &nbsp;&nbsp;author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal}, <br />
 &nbsp; month = jun, title = {Visualizing Higher-Layer Features of a Deep Network}, <br />
 &nbsp; number = {1341}, <br />
 &nbsp; year = {2009}, <br />
 &nbsp; institution = {University of Montreal}, <br />
 &nbsp; abstract = {Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2009,inproceedings] H. Larochelle, D. Erhan, and P. Vincent, &quot;Deep Learning using Robust Interdependent Codes,&quot; in <em>Proceedings of the  Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS 2009)</em>, April 16-18, 2009 2009, pp. 312-319.</div>
<p> <a href=#Larochelle-2009 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Larochelle-2009>
         <code>@INPROCEEDINGS{Larochelle-2009, <br />
 &nbsp;&nbsp;author = {Larochelle, Hugo and Erhan, Dumitru and Vincent, Pascal}, <br />
 &nbsp; month = apr, title = {Deep Learning using Robust Interdependent Codes}, <br />
 &nbsp; booktitle = {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS 2009)}, <br />
 &nbsp; year = {2009}, <br />
 &nbsp; pages = {312--319}, <br />
 &nbsp; location = {Clearwater (Florida), USA}, <br />
 &nbsp; date = "April 16-18, 2009", }</code>
    </div>
</li>
<p></p>
<li>
<div>[2008,article] Y. Bengio and J. Sénécal, &quot;Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model,&quot; <em>IEEE Transactions on Neural Networks</em>, vol. 19, iss. 4, pp. 713-722, 2008.</div>
<p> <a href=#Bengio-Senecal-2008 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Bengio-Senecal-2008>
         <code>@ARTICLE{Bengio+Senecal-2008, <br />
 &nbsp;&nbsp;author = {Bengio, Yoshua and S{\'{e}}n{\'{e}}cal, Jean-S{\'{e}}bastien}, <br />
 &nbsp; keywords = {Energy-based models, fast training, importance sampling, language modeling, Monte Carlo methods, probabilistic neural networks}, <br />
 &nbsp; title = {Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model}, <br />
 &nbsp; journal = {IEEE Transactions on Neural Networks}, <br />
 &nbsp; volume = {19}, <br />
 &nbsp; number = {4}, <br />
 &nbsp; year = {2008}, <br />
 &nbsp; pages = {713--722}, <br />
 &nbsp; abstract = {Previous work on statistical language modeling has shown that it is possible to train a feedforward neural network to approximate probabilities over sequences of words, resulting in significant error reduction when compared to standard baseline models based on -grams. However, training the neural network model with the maximum-likelihood criterion requires computations proportional to the number of words in the vocabulary. In this paper, we introduce adaptive importance sampling as a way to accelerate training of the model. The idea is to use an adaptive n-gram model to track the conditional distributions produced by the neural network. We show that a very significant speedup can be obtained on standard problems.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2008,techreport] G. Desjardins and Y. Bengio, &quot;Empirical Evaluation of Convolutional RBMs for Vision,&quot; Département d&#8217;Informatique et de Recherche Opérationnelle, Université de Montréal, 1327, 2008.</div>
<p> <a href=#Desjardins-2008 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Desjardins-2008>
         <code>@TECHREPORT{Desjardins-2008, <br />
 &nbsp;&nbsp;author = {Desjardins, Guillaume and Bengio, Yoshua}, <br />
 &nbsp; keywords = {Convolutional Architectures, Deep Networks, RBM, Vision}, <br />
 &nbsp; title = {Empirical Evaluation of Convolutional RBMs for Vision}, <br />
 &nbsp; number = {1327}, <br />
 &nbsp; year = {2008}, <br />
 &nbsp; institution = {D{\'{e}}partement d'Informatique et de Recherche Op{\'{e}}rationnelle, Universit{\'{e}} de Montr{\'{e}}al}, <br />
 &nbsp; abstract = {Convolutional Neural Networks (CNN) have had great success in machine learning tasks involving vision and represent one of the early successes of deep networks. Local receptive fields and weight sharing make their architecture ideally suited for vision tasks by helping to enforce a prior based on our knowledge of natural images. This same prior could also be applied to recent developments in the field of deep networks, in order to tailor these new architectures for artificial vision. In this context, we show how the Restricted Boltzmann Machine (RBM), the building block of Deep Belief Networks (DBN), can be adapted to operate in a convolutional manner. We compare their performance to standard fully-connected RBMs on a simple visual learning task and show that the convolutional RBMs (CRBMs) converge to smaller values of the negative likelihood function. Our experiments also indicate that CRBMs are more efficient than standard RBMs trained on small image patches, with the CRBMs having faster convergence.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2008,misc] J. Bergstra, Y. Bengio, and J. Louradour, <em>Image Classification using Higher-Order Neural Models</em>, 2008.</div>
<p> <a href=#James-al-snowbird-2008 class="toggle">bibtex</a>   <a href='http://snowbird.djvuzone.org/2007/abstracts/161.pdf' title='Go to document'><img src='http://deeplearning.net/wp-content/plugins/bib2html/external.png' width='10' height='10' alt='Go to document' /></a></p>
<div class="bibtex" id=James-al-snowbird-2008>
         <code>@MISC{James+al-snowbird-2008, <br />
 &nbsp;&nbsp;author = {Bergstra, James and Bengio, Yoshua and Louradour, Jerome}, <br />
 &nbsp; month = apr, title = {Image Classification using Higher-Order Neural Models}, <br />
 &nbsp; year = {2008}, <br />
 &nbsp; howpublished = {The Learning Workshop (Snowbird, Utah)}, <br />
 &nbsp; url = {http://snowbird.djvuzone.org/2007/abstracts/161.pdf}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2008,techreport] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol, &quot;Extracting and Composing Robust Features with Denoising Autoencoders,&quot; Département d&#8217;Informatique et Recherche Opérationnelle, Université de Montréal, 1316, 2008.</div>
<p> <a href=#Vincent-TR1316 class="toggle">bibtex</a>   <a href='http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf' title='Go to document'><img src='http://deeplearning.net/wp-content/plugins/bib2html/external.png' width='10' height='10' alt='Go to document' /></a></p>
<div class="bibtex" id=Vincent-TR1316>
         <code>@TECHREPORT{Vincent-TR1316, <br />
 &nbsp;&nbsp;author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine}, <br />
 &nbsp; month = feb, title = {Extracting and Composing Robust Features with Denoising Autoencoders}, <br />
 &nbsp; number = {1316}, <br />
 &nbsp; year = {2008}, <br />
 &nbsp; institution = {D{\'{e}}partement d'Informatique et Recherche Op{\'{e}}rationnelle, Universit{\'{e}} de Montr{\'{e}}al}, <br />
 &nbsp; url = {http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf}, <br />
 &nbsp; abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2008,inproceedings] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol, &quot;Extracting and Composing Robust Features with Denoising Autoencoders.&quot;  2008, pp. 1096-1103.</div>
<p> <a href=#VincentPLarochelleH2008 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=VincentPLarochelleH2008>
         <code>@INPROCEEDINGS{VincentPLarochelleH2008, <br />
 &nbsp;&nbsp;author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine}, <br />
 &nbsp; title = {Extracting and Composing Robust Features with Denoising Autoencoders}, <br />
 &nbsp; year = {2008}, <br />
 &nbsp; pages = {1096--1103}, <br />
 &nbsp; crossref = {ICML08}, <br />
 &nbsp; abstract = {Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2008,inproceedings] H. Larochelle and Y. Bengio, &quot;Classification using Discriminative Restricted Boltzmann Machines.&quot;  2008, pp. 536-543.</div>
<p> <a href=#Larochelle-Bengio-2008 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Larochelle-Bengio-2008>
         <code>@INPROCEEDINGS{Larochelle+Bengio-2008, <br />
 &nbsp;&nbsp;author = {Larochelle, Hugo and Bengio, Yoshua}, <br />
 &nbsp; title = {Classification using Discriminative Restricted {B}oltzmann Machines}, <br />
 &nbsp; year = {2008}, <br />
 &nbsp; pages = {536--543}, <br />
 &nbsp; location = {Helsinki, Finland}, <br />
 &nbsp; crossref = {ICML08}, <br />
 &nbsp; abstract = {Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2008,proceedings] <em>Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML&#8217;08)</em>ACM, 2008.</div>
<p> <a href=#ICML08 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=ICML08>
         <code>@PROCEEDINGS{ICML08, editor = {Cohen, William W. and McCallum, Andrew and Roweis, Sam T.}, <br />
 &nbsp; title = {Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML'08)}, <br />
 &nbsp; booktitle = {Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML'08)}, <br />
 &nbsp; year = {2008}, <br />
 &nbsp; publisher = {ACM}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2008,misc] Y. Bengio, H. Larochelle, and J. Turian, <em>Deep Woods</em>, 2008.</div>
<p> <a href=#Yoshua-al-snowbird-2008 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Yoshua-al-snowbird-2008>
         <code>@MISC{Yoshua+al-snowbird-2008, <br />
 &nbsp;&nbsp;author = {Bengio, Yoshua and Larochelle, Hugo and Turian, Joseph}, <br />
 &nbsp; title = {Deep Woods}, <br />
 &nbsp; year = {2008}, <br />
 &nbsp; howpublished = {Poster presented at the Learning@Snowbird Workshop, Snowbird, USA, 2008}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2008,phdthesis] N. Le Roux, &quot;Avancées théoriques sur la représentation et l&#8217;optimisation des réseaux de neurones,&quot; PhD Thesis , 2008.</div>
<p> <a href=#LeRoux-PhD-2008 class="toggle">bibtex</a>   <a href='http://www.iro.umontreal.ca/~lisa/pointeurs/LeRouxNicolasThese.pdf' title='Go to document'><img src='http://deeplearning.net/wp-content/plugins/bib2html/external.png' width='10' height='10' alt='Go to document' /></a></p>
<div class="bibtex" id=LeRoux-PhD-2008>
         <code>@PHDTHESIS{LeRoux-PhD-2008, <br />
 &nbsp;&nbsp;author = {Le Roux, Nicolas}, <br />
 &nbsp; month = mar, title = {Avanc{\'{e}}es th{\'{e}}oriques sur la repr{\'{e}}sentation et l'optimisation des r{\'{e}}seaux de neurones}, <br />
 &nbsp; year = {2008}, <br />
 &nbsp; school = {Universit{\'{e}} de Montr{\'{e}}al}, <br />
 &nbsp; url = {http://www.iro.umontreal.ca/~lisa/pointeurs/LeRouxNicolasThese.pdf}, <br />
 &nbsp; abstract = {Les r{\'{e}}seaux de neurones artificiels ont {\'{e}}t{\'{e}} abondamment utilis{\'{e}}s dans la communaut{\'{e}} de l'apprentissage machine depuis les ann{\'{e}}es 80. Bien qu'ils aient {\'{e}}t{\'{e}} {\'{e}}tudi{\'{e}}s pour la premi{\`{e}}re fois il y a cinquante ans par Rosenblatt [68], ils ne furent r{\'{e}}ellement populaires qu'apr{\`{e}}s l'apparition de la r{\'{e}}tropropagation du gradient, en 1986 [71]. En 1989, il a {\'{e}}t{\'{e}} prouv{\'{e}} [44] qu'une classe sp{\'{e}}cifique de r{\'{e}}seaux de neurones (les r{\'{e}}seaux de neurones {\`{a}} une couche cach{\'{e}}e) {\'{e}}tait suffisamment puissante pour pouvoir approximer presque n'importe quelle fonction avec une pr{\'{e}}cision arbitraire : le th{\'{e}}or{\`{e}}me d'approximation universelle. Toutefois, bien que ce th{\'{e}}or{\`{e}}me e{\^{u}}t pour cons{\'{e}}quence un int{\'{e}}r{\^{e}}t accru pour les r{\'{e}}seaux de neurones, il semblerait qu'aucun effort n'ait {\'{e}}t{\'{e}} fait pour profiter de cette propri{\'{e}}t{\'{e}}. En outre, l'optimisation des r{\'{e}}seaux de neurones {\`{a}} une couche cach{\'{e}}e n'est pas convexe. Cela a d{\'{e}}tourn{\'{e}} une grande partie de la communaut{\'{e}} vers d'autres algorithmes, comme par exemple les machines {\`{a}} noyau (machines {\`{a}} vecteurs de support et r{\'{e}}gression {\`{a}} noyau, entre autres). La premi{\`{e}}re partie de cette th{\`{e}}se pr{\'{e}}sentera les concepts d'apprentissage machine g{\'{e}}n{\'{e}}raux n{\'{e}}cessaires {\`{a}} la compr{\'{e}}hension des algorithmes utilis{\'{e}}s. La deuxi{\`{e}}me partie se focalisera plus sp{\'{e}}cifiquement sur les m{\'{e}}thodes {\`{a}} noyau et les r{\'{e}}seaux de neurones. La troisi{\`{e}}me partie de ce travail visera ensuite {\`{a}} {\'{e}}tudier les limitations des machines {\`{a}} noyaux et {\`{a}} comprendre les raisons pour lesquelles elles sont inadapt{\'{e}}es {\`{a}} certains probl{\`{e}}mes que nous avons {\`{a}} traiter. La quatri{\`{e}}me partie pr{\'{e}}sente une technique permettant d'optimiser les r{\'{e}}seaux de neurones {\`{a}} une couche cach{\'{e}}e de mani{\`{e}}re convexe. Bien que cette technique s'av{\`{e}}re difficilement exploitable pour des probl{\`{e}}mes de grande taille, une version approch{\'{e}}e permet d'obtenir une bonne solution dans un temps raisonnable. La cinqui{\`{e}}me partie se concentre sur les r{\'{e}}seaux de neurones {\`{a}} une couche cach{\'{e}}e infinie. Cela leur permet th{\'{e}}oriquement d'exploiter la propri{\'{e}}t{\'{e}} d'approximation universelle et ainsi d'approcher facilement une plus grande classe de fonctions. Toutefois, si ces deux variations sur les r{\'{e}}seaux de neurones {\`{a}} une couche cach{\'{e}}e leur conf{\`{e}}rent des propri{\'{e}}t{\'{e}}s int{\'{e}}ressantes, ces derniers ne peuvent extraire plus que des concepts de bas niveau. Les m{\'{e}}thodes {\`{a}} noyau souffrant des m{\^{e}}mes limites, aucun de ces deux types d'algorithmes ne peut appr{\'{e}}hender des probl{\`{e}}mes faisant appel {\`{a}} l'apprentissage de concepts de haut niveau. R{\'{e}}cemment sont apparus les Deep Belief Networks [39] qui sont des r{\'{e}}seaux de neurones {\`{a}} plusieurs couches cach{\'{e}}es entra{\^{\i}}n{\'{e}}s de mani{\`{e}}re efficace. Cette profondeur leur permet d'extraire des concepts de haut niveau et donc de r{\'{e}}aliser des t{\^{a}}ches hors de port{\'{e}}e des algorithmes conventionnels. La sixi{\`{e}}me partie {\'{e}}tudie des propri{\'{e}}t{\'{e}}s de ces r{\'{e}}seaux profonds. Les probl{\`{e}}mes que l'on rencontre actuellement n{\'{e}}cessitent non seulement des algorithmes capables d'extraire des concepts de haut niveau, mais {\'{e}}galement des m{\'{e}}thodes d'optimisation capables de traiter l'immense quantit{\'{e}} de donn{\'{e}}es parfois disponibles, si possible en temps r{\'{e}}el. La septi{\`{e}}me partie est donc la pr{\'{e}}sentation d'une nouvelle technique permettant une optimisation plus rapide.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2008,article] N. Le Roux and Y. Bengio, &quot;Representational Power of Restricted Boltzmann Machines and Deep Belief Networks,&quot; <em>Neural Computation</em>, vol. 20, iss. 6, pp. 1631-1649, 2008.</div>
<p> <a href=#LeRoux-Bengio-2008 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=LeRoux-Bengio-2008>
         <code>@ARTICLE{LeRoux-Bengio-2008, <br />
 &nbsp;&nbsp;author = {Le Roux, Nicolas and Bengio, Yoshua}, <br />
 &nbsp; month = jun, title = {Representational Power of Restricted {B}oltzmann Machines and Deep Belief Networks}, <br />
 &nbsp; journal = {Neural Computation}, <br />
 &nbsp; volume = {20}, <br />
 &nbsp; number = {6}, <br />
 &nbsp; year = {2008}, <br />
 &nbsp; pages = {1631--1649}, <br />
 &nbsp; abstract = {Deep Belief Networks (DBN) are generative neural network models with many layers of hidden explanatory factors, recently introduced by Hinton et al., along with a greedy layer-wise unsupervised learning algorithm. The building block of a DBN is a probabilistic model called a Restricted Boltzmann Machine (RBM), used to represent one layer of the model. Restricted Boltzmann Machines are interesting because inference is easy in them, and because they have been successfully used as building blocks for training deeper models. We first prove that adding hidden units yields strictly improved modelling power, while a second theorem shows that RBMs are universal approximators of discrete distributions. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2007,proceedings] <em>Proceedings of the 24th International Conference on Machine Learning (ICML&#8217;07)</em>ACM, 2007.</div>
<p> <a href=#ICML07 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=ICML07>
         <code>@PROCEEDINGS{ICML07, editor = {Ghahramani, Zoubin}, <br />
 &nbsp; title = {Proceedings of the 24th International Conference on Machine Learning (ICML'07)}, <br />
 &nbsp; booktitle = {Proceedings of the 24th International Conference on Machine Learning (ICML'07)}, <br />
 &nbsp; year = {2007}, <br />
 &nbsp; publisher = {ACM}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2007,techreport] Y. Bengio and O. Delalleau, &quot;Justifying and Generalizing Contrastive Divergence,&quot; Département d&#8217;Informatique et Recherche Opérationnelle, Université de Montréal, 1311, 2007.</div>
<p> <a href=#Bengio-Delalleau-TR2007 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=Bengio-Delalleau-TR2007>
         <code>@TECHREPORT{Bengio+Delalleau-TR2007, <br />
 &nbsp;&nbsp;author = {Bengio, Yoshua and Delalleau, Olivier}, <br />
 &nbsp; keywords = {Contrastive Divergence, Restricted Boltzmann Machine}, <br />
 &nbsp; title = {Justifying and Generalizing Contrastive Divergence}, <br />
 &nbsp; number = {1311}, <br />
 &nbsp; year = {2007}, <br />
 &nbsp; institution = {D{\'{e}}partement d'Informatique et Recherche Op{\'{e}}rationnelle, Universit{\'{e}} de Montr{\'{e}}al}, <br />
 &nbsp; abstract = {We study an expansion of the log-likelihood in undirected graphical models such as the Restricted Boltzmann Machine (RBM), where each term in the expansion is associated with a sample in a Gibbs chain alternating between two random variables (the visible vector and the hidden vector, in RBMs). We are particularly interested in estimators of the gradient of the log-likelihood obtained through this expansion. We show that its terms converge to zero, justifying the use of a truncation, i.e. running only a short Gibbs chain, which is the main idea behind the Contrastive Divergence approximation of the log-likelihood gradient. By truncating even more, we obtain a stochastic reconstruction error, related through a mean-field approximation to the reconstruction error often used to train autoassociators and stacked auto-associators. The derivation is not specific to the particular parametric forms used in RBMs, and only requires convergence of the Gibbs chain.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2007,inproceedings] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio, &quot;An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation.&quot;  2007, pp. 473-480.</div>
<p> <a href=#LarochelleH2007 class="toggle">bibtex</a>   <a href='http://oregonstate.edu/conferences/icml2007/paperlist.html' title='Go to document'><img src='http://deeplearning.net/wp-content/plugins/bib2html/external.png' width='10' height='10' alt='Go to document' /></a></p>
<div class="bibtex" id=LarochelleH2007>
         <code>@INPROCEEDINGS{LarochelleH2007, <br />
 &nbsp;&nbsp;author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua}, <br />
 &nbsp; title = {An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation}, <br />
 &nbsp; year = {2007}, <br />
 &nbsp; pages = {473--480}, <br />
 &nbsp; location = {Corvallis, OR}, <br />
 &nbsp; url = {http://oregonstate.edu/conferences/icml2007/paperlist.html}, <br />
 &nbsp; doi = {http://doi.acm.org/10.1145/1273496.1273556}, <br />
 &nbsp; crossref = {ICML07}, <br />
 &nbsp; abstract = {Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2007,incollection] Y. Bengio and Y. LeCun, &quot;Scaling Learning Algorithms towards AI,&quot; , Bottou, ., Chapelle, O., DeCoste, D., and Weston, J., Eds., MIT Press, 2007.</div>
<p> <a href=#Bengio-chapter2007 class="toggle">bibtex</a>   <a href='http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf' title='Go to document'><img src='http://deeplearning.net/wp-content/plugins/bib2html/external.png' width='10' height='10' alt='Go to document' /></a></p>
<div class="bibtex" id=Bengio-chapter2007>
         <code>@INCOLLECTION{Bengio+chapter2007, <br />
 &nbsp;&nbsp;author = {Bengio, Yoshua and {LeCun}, <br />
 &nbsp; Yann}, <br />
 &nbsp; editor = {Bottou, {L{\'{e}}on} and Chapelle, Olivier and DeCoste, D. and Weston, J.}, <br />
 &nbsp; title = {Scaling Learning Algorithms towards {AI}}, <br />
 &nbsp; booktitle = {Large Scale Kernel Machines}, <br />
 &nbsp; year = {2007}, <br />
 &nbsp; publisher = {MIT Press}, <br />
 &nbsp; url = {http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf}, <br />
 &nbsp; abstract = {One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.}, <br />
 &nbsp; cat={B}, <br />
 &nbsp;topics={HighDimensional}, <br />
 &nbsp; }</code>
    </div>
</li>
<p></p>
<li>
<div>[2007,incollection] Y. Bengio, &quot;On the Challenge of Learning Complex Functions,&quot; , Cisek, P., Kalaska, J., and Drew, T., Eds., Elsevier, 2007.</div>
<p> <a href=#Bengio-2007 class="toggle">bibtex</a>   <a href='http://www.iro.umontreal.ca/~lisa/pointeurs/PBR_chapter.pdf' title='Go to document'><img src='http://deeplearning.net/wp-content/plugins/bib2html/external.png' width='10' height='10' alt='Go to document' /></a></p>
<div class="bibtex" id=Bengio-2007>
         <code>@INCOLLECTION{Bengio-2007, <br />
 &nbsp;&nbsp;author = {Bengio, Yoshua}, <br />
 &nbsp; editor = {Cisek, Paul and Kalaska, John and Drew, Trevor}, <br />
 &nbsp; title = {On the Challenge of Learning Complex Functions}, <br />
 &nbsp; booktitle = {Computational Neuroscience: Theoretical Insights into Brain Function}, <br />
 &nbsp; series = {Progress in Brain Research}, <br />
 &nbsp; year = {2007}, <br />
 &nbsp; publisher = {Elsevier}, <br />
 &nbsp; url = {http://www.iro.umontreal.ca/~lisa/pointeurs/PBR_chapter.pdf}, <br />
 &nbsp; abstract = {A common goal of computational neuroscience and of artificial intelligence research based on statistical learning algorithms is the discovery and understanding of computational principles that could explain what we consider adaptive intelligence, in animals as well as in machines. This chapter focuses on what is required for the learning of complex behaviors. We believe it involves the learning of highly varying functions, in a mathematical sense. We bring forward two types of arguments which convey the message that many currently popular machine learning approaches to learning flexible functions have fundamental limitations that render them inappropriate for learning highly varying functions. The first issue concerns the representation of such functions with what we call shallow model architectures. We discuss limitations of shallow architectures, such as so-called kernel machines, boosting algorithms, and one-hidden-layer artificial neural networks. The second issue is more focused and concerns kernel machines with a local kernel (the type used most often in practice), that act like a collection of template matching units. We present mathematical results on such computational architectures showing that they have a limitation similar to those already proved for older non-parametric methods, and connected to the so-called curse of dimensionality. Though it has long been believed that efficient learning in deep architectures is difficult, recently proposed computational principles for learning in deep architectures may offer a breakthrough.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2007,techreport] N. Le Roux and Y. Bengio, &quot;Representational Power of Restricted Boltzmann Machines and Deep Belief Networks,&quot; Département d&#8217;Informatique et de Recherche Opérationnelle, Université de Montréal, Montréal (QC) Canada, 1294, 2007.</div>
<p> <a href=#LeRoux-Bengio-2007-TR class="toggle">bibtex</a>  </p>
<div class="bibtex" id=LeRoux-Bengio-2007-TR>
         <code>@TECHREPORT{LeRoux-Bengio-2007-TR, <br />
 &nbsp;&nbsp;author = {Le Roux, Nicolas and Bengio, Yoshua}, <br />
 &nbsp; month = apr, title = {Representational Power of Restricted {B}oltzmann Machines and Deep Belief Networks}, <br />
 &nbsp; number = {1294}, <br />
 &nbsp; year = {2007}, <br />
 &nbsp; institution = {D{\'{e}}partement d'Informatique et de Recherche Op{\'{e}}rationnelle, Universit{\'{e}} de Montr{\'{e}}al}, <br />
 &nbsp; address = {Montr{\'{e}}al (QC) Canada}, <br />
 &nbsp; abstract = {Deep Belief Networks (DBN) are generative neural network models with many layers of hidden explanatory factors, recently introduced by Hinton et al., along with a greedy layer-wise unsupervised learning algorithm. The building block of a DBN is a probabilistic model called a Restricted Boltzmann Machine (RBM), used to represent one layer of the model. Restricted Boltzmann Machines are interesting because inference is easy in them, and because they have been successfully used as building blocks for training deeper models. We first prove that adding hidden units yields strictly improved modeling power, while a second theorem shows that RBMs are universal approximators of discrete distributions. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2007,techreport] Y. Bengio, &quot;Learning deep architectures for AI,&quot; Dept. IRO, Universite de Montreal, 1312, 2007.</div>
<p> <a href=#Bengio-TR1312 class="toggle">bibtex</a>   <a href='http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf' title='Go to document'><img src='http://deeplearning.net/wp-content/plugins/bib2html/external.png' width='10' height='10' alt='Go to document' /></a></p>
<div class="bibtex" id=Bengio-TR1312>
         <code>@TECHREPORT{Bengio-TR1312, <br />
 &nbsp;&nbsp;author = {Bengio, Yoshua}, <br />
 &nbsp; title = {Learning deep architectures for AI}, <br />
 &nbsp; number = {1312}, <br />
 &nbsp; year = {2007}, <br />
 &nbsp; institution = {Dept. IRO, Universite de Montreal}, <br />
 &nbsp; note = {Preliminary version of journal article with the same title appearing in Foundations and Trends in Machine Learning (2009)}, <br />
 &nbsp; url = {http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf}, <br />
 &nbsp; abstract = {Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures and in particular for those based on unsupervised learning such as Deep Belief Networks, using as building blocks single-layer models such as Restricted Boltzmann Machines.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2007,inproceedings] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, &quot;Greedy Layer-Wise Training of Deep Networks.&quot;  2007, pp. 153-160.</div>
<p> <a href=#Bengio-nips-2006 class="toggle">bibtex</a>   <a href='http://www.iro.umontreal.ca/~lisa/pointeurs/BengioNips2006All.pdf' title='Go to document'><img src='http://deeplearning.net/wp-content/plugins/bib2html/external.png' width='10' height='10' alt='Go to document' /></a></p>
<div class="bibtex" id=Bengio-nips-2006>
         <code>@INPROCEEDINGS{Bengio-nips-2006, <br />
 &nbsp;&nbsp;author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo}, <br />
 &nbsp; title = {Greedy Layer-Wise Training of Deep Networks}, <br />
 &nbsp; year = {2007}, <br />
 &nbsp; pages = {153--160}, <br />
 &nbsp; url = {http://www.iro.umontreal.ca/~lisa/pointeurs/BengioNips2006All.pdf}, <br />
 &nbsp; crossref = {NIPS19}, <br />
 &nbsp; abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[2007,techreport] Y. Bengio, O. Delalleau, and C. Simard, &quot;Decision Trees do not Generalize to New Variations,&quot; Département d&#8217;informatique et recherche opérationnelle, Université de Montréal, 1304, 2007.</div>
<p> <a href=#Bengio-al-treecurse-2007 class="toggle">bibtex</a>   <a href='http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+al-tr1304.pdf' title='Go to document'><img src='http://deeplearning.net/wp-content/plugins/bib2html/external.png' width='10' height='10' alt='Go to document' /></a></p>
<div class="bibtex" id=Bengio-al-treecurse-2007>
         <code>@TECHREPORT{Bengio+al-treecurse-2007, <br />
 &nbsp;&nbsp;author = {Bengio, Yoshua and Delalleau, Olivier and Simard, Clarence}, <br />
 &nbsp; month = jun, title = {Decision Trees do not Generalize to New Variations}, <br />
 &nbsp; number = {1304}, <br />
 &nbsp; year = {2007}, <br />
 &nbsp; institution = {D{\'{e}}partement d'informatique et recherche op{\'{e}}rationnelle, Universit{\'{e}} de Montr{\'{e}}al}, <br />
 &nbsp; url = {http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+al-tr1304.pdf}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[-1,inproceedings] &quot;Advances in Neural Information Processing Systems 19 (NIPS&#8217;06),&quot; in <em>Advances in Neural Information Processing Systems 19 (NIPS&#8217;06)</em>,  -1.</div>
<p> <a href=#NIPS19 class="toggle">bibtex</a>  </p>
<div class="bibtex" id=NIPS19>
         <code>@INPROCEEDINGS{NIPS19, editor = {{Sch{\"{o}}lkopf}, <br />
 &nbsp; Bernhard and Platt, John and Hoffman, Thomas}, <br />
 &nbsp; title = {Advances in Neural Information Processing Systems 19 (NIPS'06)}, <br />
 &nbsp; booktitle = {Advances in Neural Information Processing Systems 19 (NIPS'06)}, <br />
 &nbsp; year = {-1}, <br />
 &nbsp; publisher = {MIT Press}<br />
}</code>
    </div>
</li>
<p></p>
<li>
<div>[-1,article] ,&quot; <em>Journal of Machine Learning Research</em>, -1.</div>
<p> <a href=#JMLR class="toggle">bibtex</a>  </p>
<div class="bibtex" id=JMLR>
         <code>@ARTICLE{JMLR, journal = {Journal of Machine Learning Research}, <br />
 &nbsp; year = {-1}<br />
}</code>
    </div>
</li>
</ul>
</div>						<div class="post-footer"> Last modified on September 19, 2014, at 10:28 am by Caglar Gulcehre</div>		</div><!-- / Post -->	
						
	
					
    

</td>
<!-- / Main Column -->

<!-- Right Inner Sidebar -->

<!-- Right Sidebar -->
<td id="right">

			<div id="recent-posts-4" class="widget widget_recent_entries">		<div class="widget-title"><h3>Recent Posts</h3></div>		<ul>
					<li>
				<a href="http://deeplearning.net/2016/07/14/4153924/">MILA is Hiring Two Software Engineers</a>
						</li>
					<li>
				<a href="http://deeplearning.net/2015/12/20/openai-a-new-non-profit-ai-company/">OpenAI: A new non-profit AI company</a>
						</li>
					<li>
				<a href="http://deeplearning.net/2015/12/01/conference-on-the-economics-of-machine-intelligence-dec-15/">Conference on the Economics of Machine Intelligence-Dec 15</a>
						</li>
					<li>
				<a href="http://deeplearning.net/2015/11/26/open-discussions-for-iclr-2016-is-now-open/">Open Discussion of ICLR 2016 Papers is Now Open</a>
						</li>
					<li>
				<a href="http://deeplearning.net/2015/10/16/software-developer-position-at-mila/">Software Developer Position at MILA</a>
						</li>
				</ul>
		</div>		<div id="linkcat-5" class="widget widget_links"><div class="widget-title"><h3>Links</h3></div>
	<ul class='xoxo blogroll'>
<li><a href="https://blocks.readthedocs.org/en/latest/" title="A Theano based deep learning framework." target="_top">Blocks</a></li>
<li><a href="http://www.computervisiontalks.com/tag/deep-learning/" title="Deep Learning related Computer Vision Talks" target="_blank">Deep Learning Computer Vision Talks</a></li>
<li><a href="https://plus.google.com/communities/112866381580457264725" rel="me" title="Google Plus Deep Learning Community" target="_blank">Google Plus Deep Learning Community</a></li>
<li><a href="http://metaoptimize.com/qa/" target="_blank">Metaoptimize QA/Forum</a></li>
<li><a href="http://deeplearning.net/software/pylearn2/" target="_blank">Pylearn2</a></li>
<li><a href="http://www.reddit.com/r/MachineLearning">Reddit Machine Learning</a></li>
<li><a href="http://news.startup.ml/" target="_blank">Social News for Deep Learning</a></li>
<li><a href="http://deeplearning.net/software/theano" title="CPU/GPU symbolic expression compiler in python (from LISA lab at University of Montreal)  ">Theano</a></li>

	</ul>
</div>

</td>
<!-- / Right Sidebar -->

</tr>
<!-- / Main Body -->


</table><!-- / layout -->
</div><!-- / container -->
</div><!-- / wrapper -->

</body>
</html>
